{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5436e99-1d0e-44ec-b683-987f74179230",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Defaulting to only available Python version: py3\n",
      "INFO:sagemaker.image_uris:Defaulting to only supported image scope: cpu.\n",
      "WARNING:sagemaker:instance_type is a PipelineVariable (<class 'sagemaker.workflow.parameters.ParameterString'>). Its interpreted value in execution time should not be of GPU types since GPU training is not supported for Scikit-Learn.\n",
      "WARNING:sagemaker.workflow.utilities:The input argument instance_type of function (sagemaker.image_uris.retrieve) is a pipeline variable (<class 'sagemaker.workflow.parameters.ParameterString'>), which is interpreted in pipeline execution time only. As the function needs to evaluate the argument value in SDK compile time, the default_value of this Parameter object will be used to override it. Please make sure the default_value is valid.\n",
      "WARNING:sagemaker.workflow.utilities:The input argument instance_type of function (sagemaker.image_uris.get_training_image_uri) is a pipeline variable (<class 'sagemaker.workflow.parameters.ParameterString'>), which is interpreted in pipeline execution time only. As the function needs to evaluate the argument value in SDK compile time, the default_value of this Parameter object will be used to override it. Please make sure the default_value is valid.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Properties' object has no attribute 'decode'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 370\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[38;5;66;03m# モデルステップの定義（モデルの登録）\u001b[39;00m\n\u001b[1;32m    360\u001b[0m model \u001b[38;5;241m=\u001b[39m Model(\n\u001b[1;32m    361\u001b[0m     image_uri\u001b[38;5;241m=\u001b[39msklearn_estimator\u001b[38;5;241m.\u001b[39mtraining_image_uri(),\n\u001b[1;32m    362\u001b[0m     model_data\u001b[38;5;241m=\u001b[39mtraining_step\u001b[38;5;241m.\u001b[39mproperties\u001b[38;5;241m.\u001b[39mModelArtifacts\u001b[38;5;241m.\u001b[39mS3ModelArtifacts,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    365\u001b[0m     sagemaker_session\u001b[38;5;241m=\u001b[39msagemaker_session,\n\u001b[1;32m    366\u001b[0m )\n\u001b[1;32m    368\u001b[0m model_step \u001b[38;5;241m=\u001b[39m ModelStep(\n\u001b[1;32m    369\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRegisterModel\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m--> 370\u001b[0m     step_args\u001b[38;5;241m=\u001b[39m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mml.m5.large\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    371\u001b[0m )\n\u001b[1;32m    373\u001b[0m \u001b[38;5;66;03m# 条件ステップの定義\u001b[39;00m\n\u001b[1;32m    374\u001b[0m cond_gte \u001b[38;5;241m=\u001b[39m ConditionGreaterThanOrEqualTo(\n\u001b[1;32m    375\u001b[0m     left\u001b[38;5;241m=\u001b[39mJsonGet(\n\u001b[1;32m    376\u001b[0m         step_name\u001b[38;5;241m=\u001b[39mevaluation_step\u001b[38;5;241m.\u001b[39mname,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    380\u001b[0m     right\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.8\u001b[39m,\n\u001b[1;32m    381\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker/workflow/pipeline_context.py:346\u001b[0m, in \u001b[0;36mrunnable_by_pipeline.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    342\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m context\n\u001b[1;32m    344\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _StepArguments(retrieve_caller_name(self_instance), run_func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 346\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrun_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker/model.py:658\u001b[0m, in \u001b[0;36mModel.create\u001b[0;34m(self, instance_type, accelerator_type, serverless_inference_config, tags, accept_eula, model_reference_arn)\u001b[0m\n\u001b[1;32m    624\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Create a SageMaker Model Entity\u001b[39;00m\n\u001b[1;32m    625\u001b[0m \n\u001b[1;32m    626\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    655\u001b[0m \u001b[38;5;124;03m    :class:`~sagemaker.workflow.pipeline_context.PipelineSession`\u001b[39;00m\n\u001b[1;32m    656\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    657\u001b[0m \u001b[38;5;66;03m# TODO: we should replace _create_sagemaker_model() with create()\u001b[39;00m\n\u001b[0;32m--> 658\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_sagemaker_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    659\u001b[0m \u001b[43m    \u001b[49m\u001b[43minstance_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minstance_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    660\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccelerator_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccelerator_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    661\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mformat_tags\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtags\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    662\u001b[0m \u001b[43m    \u001b[49m\u001b[43mserverless_inference_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserverless_inference_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    663\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_eula\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_eula\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    664\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_reference_arn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_reference_arn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    665\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker/model.py:942\u001b[0m, in \u001b[0;36mModel._create_sagemaker_model\u001b[0;34m(self, instance_type, accelerator_type, tags, serverless_inference_config, accept_eula, model_reference_arn)\u001b[0m\n\u001b[1;32m    930\u001b[0m container_def \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_container_def(\n\u001b[1;32m    931\u001b[0m     instance_type,\n\u001b[1;32m    932\u001b[0m     accelerator_type\u001b[38;5;241m=\u001b[39maccelerator_type,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    935\u001b[0m     model_reference_arn\u001b[38;5;241m=\u001b[39mmodel_reference_arn,\n\u001b[1;32m    936\u001b[0m )\n\u001b[1;32m    938\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msagemaker_session, PipelineSession):\n\u001b[1;32m    939\u001b[0m     \u001b[38;5;66;03m# _base_name, model_name are not needed under PipelineSession.\u001b[39;00m\n\u001b[1;32m    940\u001b[0m     \u001b[38;5;66;03m# the model_data may be Pipeline variable\u001b[39;00m\n\u001b[1;32m    941\u001b[0m     \u001b[38;5;66;03m# which may break the _base_name generation\u001b[39;00m\n\u001b[0;32m--> 942\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_base_name_if_needed\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    943\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimage_uri\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontainer_def\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mImage\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    944\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscript_uri\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msource_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    945\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_uri\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_model_uri\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    947\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_model_name_if_needed()\n\u001b[1;32m    949\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_sagemaker_session_if_does_not_exist(instance_type)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker/model.py:998\u001b[0m, in \u001b[0;36mModel._ensure_base_name_if_needed\u001b[0;34m(self, image_uri, script_uri, model_uri)\u001b[0m\n\u001b[1;32m    991\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Create a base name from the image URI if there is no model name provided.\u001b[39;00m\n\u001b[1;32m    992\u001b[0m \n\u001b[1;32m    993\u001b[0m \u001b[38;5;124;03mIf a JumpStart script or model uri is used, select the JumpStart base name.\u001b[39;00m\n\u001b[1;32m    994\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    995\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    996\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_base_name \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    997\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_base_name\n\u001b[0;32m--> 998\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mget_jumpstart_base_name_if_jumpstart_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscript_uri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_uri\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    999\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m utils\u001b[38;5;241m.\u001b[39mbase_name_from_image(image_uri, default_base_name\u001b[38;5;241m=\u001b[39mModel\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m   1000\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker/jumpstart/utils.py:383\u001b[0m, in \u001b[0;36mget_jumpstart_base_name_if_jumpstart_model\u001b[0;34m(*uris)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return default JumpStart base name if a URI belongs to JumpStart.\u001b[39;00m\n\u001b[1;32m    376\u001b[0m \n\u001b[1;32m    377\u001b[0m \u001b[38;5;124;03mIf no URIs belong to JumpStart, return None.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;124;03m    *uris (Optional[str]): URI to test for association with JumpStart.\u001b[39;00m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m uri \u001b[38;5;129;01min\u001b[39;00m uris:\n\u001b[0;32m--> 383\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mis_jumpstart_model_uri\u001b[49m\u001b[43m(\u001b[49m\u001b[43muri\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    384\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m constants\u001b[38;5;241m.\u001b[39mJUMPSTART_RESOURCE_BASE_NAME\n\u001b[1;32m    385\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker/jumpstart/utils.py:292\u001b[0m, in \u001b[0;36mis_jumpstart_model_uri\u001b[0;34m(uri)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns True if URI corresponds to a JumpStart-hosted model.\u001b[39;00m\n\u001b[1;32m    286\u001b[0m \n\u001b[1;32m    287\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;124;03m    uri (Optional[str]): uri for inference/training job.\u001b[39;00m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    291\u001b[0m bucket \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 292\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43murlparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43muri\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mscheme \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ms3\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    293\u001b[0m     bucket, _ \u001b[38;5;241m=\u001b[39m parse_s3_url(uri)\n\u001b[1;32m    295\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m bucket \u001b[38;5;129;01min\u001b[39;00m constants\u001b[38;5;241m.\u001b[39mJUMPSTART_GATED_AND_PUBLIC_BUCKET_NAME_SET\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/urllib/parse.py:399\u001b[0m, in \u001b[0;36murlparse\u001b[0;34m(url, scheme, allow_fragments)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21murlparse\u001b[39m(url, scheme\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, allow_fragments\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    380\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Parse a URL into 6 components:\u001b[39;00m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;124;03m    <scheme>://<netloc>/<path>;<params>?<query>#<fragment>\u001b[39;00m\n\u001b[1;32m    382\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;124;03m    Note that % escapes are not expanded.\u001b[39;00m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 399\u001b[0m     url, scheme, _coerce_result \u001b[38;5;241m=\u001b[39m \u001b[43m_coerce_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheme\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    400\u001b[0m     splitresult \u001b[38;5;241m=\u001b[39m urlsplit(url, scheme, allow_fragments)\n\u001b[1;32m    401\u001b[0m     scheme, netloc, url, query, fragment \u001b[38;5;241m=\u001b[39m splitresult\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/urllib/parse.py:136\u001b[0m, in \u001b[0;36m_coerce_args\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m str_input:\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m args \u001b[38;5;241m+\u001b[39m (_noop,)\n\u001b[0;32m--> 136\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_decode_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m (_encode_result,)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/urllib/parse.py:120\u001b[0m, in \u001b[0;36m_decode_args\u001b[0;34m(args, encoding, errors)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_decode_args\u001b[39m(args, encoding\u001b[38;5;241m=\u001b[39m_implicit_encoding,\n\u001b[1;32m    119\u001b[0m                        errors\u001b[38;5;241m=\u001b[39m_implicit_errors):\n\u001b[0;32m--> 120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/urllib/parse.py:120\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_decode_args\u001b[39m(args, encoding\u001b[38;5;241m=\u001b[39m_implicit_encoding,\n\u001b[1;32m    119\u001b[0m                        errors\u001b[38;5;241m=\u001b[39m_implicit_errors):\n\u001b[0;32m--> 120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m(encoding, errors) \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m args)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Properties' object has no attribute 'decode'"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.session import Session\n",
    "from sagemaker.workflow.parameters import ParameterString\n",
    "from sagemaker.workflow.steps import ProcessingStep, TrainingStep\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "from sagemaker.processing import ScriptProcessor, ProcessingInput, ProcessingOutput\n",
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "from sagemaker.workflow.condition_step import ConditionStep\n",
    "from sagemaker.workflow.conditions import ConditionGreaterThanOrEqualTo\n",
    "from sagemaker.workflow.functions import JsonGet\n",
    "from sagemaker.workflow.model_step import ModelStep\n",
    "from sagemaker.workflow.properties import PropertyFile\n",
    "from sagemaker.model import Model\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# セッションとロールの設定\n",
    "sagemaker_session = sagemaker.session.Session()\n",
    "role = get_execution_role()\n",
    "region = sagemaker_session.boto_region_name\n",
    "default_bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "# パラメータの定義\n",
    "processing_instance_type = ParameterString(name=\"ProcessingInstanceType\", default_value=\"ml.m5.xlarge\")\n",
    "training_instance_type = ParameterString(name=\"TrainingInstanceType\", default_value=\"ml.m5.xlarge\")\n",
    "model_approval_status = ParameterString(name=\"ModelApprovalStatus\", default_value=\"Approved\")\n",
    "\n",
    "# データのS3パスの定義\n",
    "input_data_uri = f's3://{default_bucket}/lightgbm-pipeline/input/data.csv'\n",
    "output_data_uri = f's3://{default_bucket}/lightgbm-pipeline/output'\n",
    "\n",
    "# サンプルデータの作成とS3へのアップロード\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X, y = make_classification(\n",
    "    n_samples=1000, n_features=20, n_informative=15, n_redundant=5, n_classes=2, random_state=42\n",
    ")\n",
    "df = pd.DataFrame(X)\n",
    "df['target'] = y\n",
    "\n",
    "# データをS3にアップロード\n",
    "os.makedirs('data', exist_ok=True)\n",
    "df.to_csv('data/data.csv', index=False)\n",
    "sagemaker_session.upload_data(path='data/data.csv', bucket=default_bucket, key_prefix='lightgbm-pipeline/input')\n",
    "\n",
    "# 前処理スクリプトの作成\n",
    "processing_script = \"\"\"\n",
    "import argparse\n",
    "import os\n",
    "import pandas as pd\n",
    "import boto3\n",
    "import logging\n",
    "from sklearn.model_selection import train_test_split\n",
    "from io import StringIO\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    logger.info(\"前処理を開始します。\")\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--input-bucket\", type=str)\n",
    "    parser.add_argument(\"--input-key\", type=str)\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    logger.info(\"S3からデータを読み込んでいます。\")\n",
    "    s3 = boto3.client('s3')\n",
    "    response = s3.get_object(Bucket=args.input_bucket, Key=args.input_key)\n",
    "    df = pd.read_csv(StringIO(response['Body'].read().decode('utf-8')))\n",
    "\n",
    "    logger.info(f\"入力データの形状: {df.shape}\")\n",
    "\n",
    "    logger.info(\"データを訓練セットと検証セットに分割します。\")\n",
    "    train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "    os.makedirs('/opt/ml/processing/train', exist_ok=True)\n",
    "    os.makedirs('/opt/ml/processing/validation', exist_ok=True)\n",
    "\n",
    "    logger.info(\"訓練データを保存します。\")\n",
    "    train_df.to_csv('/opt/ml/processing/train/train.csv', index=False)\n",
    "\n",
    "    logger.info(\"検証データを保存します。\")\n",
    "    val_df.to_csv('/opt/ml/processing/validation/validation.csv', index=False)\n",
    "\n",
    "    logger.info(\"前処理が完了しました。\")\n",
    "\"\"\"\n",
    "with open('preprocessing.py', 'w') as f:\n",
    "    f.write(processing_script)\n",
    "\n",
    "# 前処理ステップの定義\n",
    "script_processor = ScriptProcessor(\n",
    "    image_uri=sagemaker.image_uris.retrieve(framework='sklearn', region=region, version='0.23-1'),\n",
    "    command=['python3'],\n",
    "    instance_type=processing_instance_type,\n",
    "    instance_count=1,\n",
    "    base_job_name='lightgbm-preprocessing',\n",
    "    role=role,\n",
    "    sagemaker_session=sagemaker_session\n",
    ")\n",
    "\n",
    "processing_step = ProcessingStep(\n",
    "    name='Preprocessing',\n",
    "    processor=script_processor,\n",
    "    inputs=[],\n",
    "    outputs=[\n",
    "        ProcessingOutput(\n",
    "            output_name='train_data',\n",
    "            source='/opt/ml/processing/train',\n",
    "            destination=f's3://{default_bucket}/lightgbm-pipeline/output/train'\n",
    "        ),\n",
    "        ProcessingOutput(\n",
    "            output_name='validation_data',\n",
    "            source='/opt/ml/processing/validation',\n",
    "            destination=f's3://{default_bucket}/lightgbm-pipeline/output/validation'\n",
    "        ),\n",
    "    ],\n",
    "    code='preprocessing.py',\n",
    "    job_arguments=[\n",
    "        \"--input-bucket\", default_bucket,\n",
    "        \"--input-key\", \"lightgbm-pipeline/input/data.csv\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 学習スクリプトの作成\n",
    "training_script = \"\"\"\n",
    "import argparse\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# 必要なパッケージをインストール\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"lightgbm\", \"optuna\"])\n",
    "\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import optuna\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def objective(trial, X_train, y_train, X_val, y_val):\n",
    "    params = {\n",
    "        'objective': 'binary',\n",
    "        'metric': 'auc',\n",
    "        'verbosity': -1,\n",
    "        'boosting_type': 'gbdt',\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 20, 150),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 1e-4, 1e-1),\n",
    "        'feature_fraction': trial.suggest_uniform('feature_fraction', 0.6, 1.0),\n",
    "        'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.6, 1.0),\n",
    "        'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n",
    "        'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-4, 10.0),\n",
    "        'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-4, 10.0),\n",
    "    }\n",
    "\n",
    "    train_dataset = lgb.Dataset(X_train, label=y_train)\n",
    "    val_dataset = lgb.Dataset(X_val, label=y_val)\n",
    "\n",
    "    model = lgb.train(params, train_dataset, valid_sets=[val_dataset], early_stopping_rounds=10, verbose_eval=False)\n",
    "    y_pred = model.predict(X_val)\n",
    "    auc = roc_auc_score(y_val, y_pred)\n",
    "    return auc\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--n-trials\", type=int, default=20)\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    print(\"訓練データを読み込んでいます。\")\n",
    "    train_df = pd.read_csv('/opt/ml/input/data/train/train.csv')\n",
    "\n",
    "    print(\"検証データを読み込んでいます。\")\n",
    "    val_df = pd.read_csv('/opt/ml/input/data/validation/validation.csv')\n",
    "\n",
    "    y_train = train_df.pop('target')\n",
    "    X_train = train_df\n",
    "    y_val = val_df.pop('target')\n",
    "    X_val = val_df\n",
    "\n",
    "    # Optunaによるハイパーパラメータ最適化\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(lambda trial: objective(trial, X_train, y_train, X_val, y_val), n_trials=args.n_trials)\n",
    "\n",
    "    print('Best trial:')\n",
    "    trial = study.best_trial\n",
    "    print('  AUC: {}'.format(trial.value))\n",
    "    print('  Params: ')\n",
    "    for key, value in trial.params.items():\n",
    "        print('    {}: {}'.format(key, value))\n",
    "\n",
    "    # 最適なハイパーパラメータでモデルを再学習\n",
    "    best_params = trial.params\n",
    "    best_params['objective'] = 'binary'\n",
    "    best_params['metric'] = 'auc'\n",
    "    best_params['verbosity'] = -1\n",
    "    best_params['boosting_type'] = 'gbdt'\n",
    "\n",
    "    train_dataset = lgb.Dataset(X_train, label=y_train)\n",
    "    val_dataset = lgb.Dataset(X_val, label=y_val)\n",
    "\n",
    "    model = lgb.train(best_params, train_dataset, valid_sets=[val_dataset], early_stopping_rounds=10)\n",
    "\n",
    "    # モデルを保存\n",
    "    model_dir = os.environ.get('SM_MODEL_DIR')\n",
    "    model.save_model(os.path.join(model_dir, \"model.txt\"))\n",
    "\"\"\"\n",
    "with open('train.py', 'w') as f:\n",
    "    f.write(training_script)\n",
    "\n",
    "# 学習ステップの定義\n",
    "sklearn_estimator = SKLearn(\n",
    "    entry_point='train.py',\n",
    "    role=role,\n",
    "    instance_type=training_instance_type,\n",
    "    instance_count=1,\n",
    "    framework_version='0.23-1',\n",
    "    base_job_name='lightgbm-training',\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    hyperparameters={\n",
    "        'n-trials': 50,  # Optunaの試行回数\n",
    "    },\n",
    ")\n",
    "\n",
    "training_step = TrainingStep(\n",
    "    name='Training',\n",
    "    estimator=sklearn_estimator,\n",
    "    inputs={\n",
    "        'train': sagemaker.inputs.TrainingInput(\n",
    "            s3_data=processing_step.properties.ProcessingOutputConfig.Outputs['train_data'].S3Output.S3Uri,\n",
    "            content_type='text/csv'\n",
    "        ),\n",
    "        'validation': sagemaker.inputs.TrainingInput(\n",
    "            s3_data=processing_step.properties.ProcessingOutputConfig.Outputs['validation_data'].S3Output.S3Uri,\n",
    "            content_type='text/csv'\n",
    "        ),\n",
    "    },\n",
    ")\n",
    "\n",
    "# 評価スクリプトの作成\n",
    "evaluation_script = \"\"\"\n",
    "import argparse\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"lightgbm\"])\n",
    "\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import json\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--output-dir\", type=str, default=\"/opt/ml/processing/evaluation\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    print(\"モデルを読み込んでいます。\")\n",
    "    model = lgb.Booster(model_file=os.path.join('/opt/ml/processing/model', 'model.txt'))\n",
    "\n",
    "    print(\"検証データを読み込んでいます。\")\n",
    "    val_df = pd.read_csv('/opt/ml/processing/validation/validation.csv')\n",
    "    y_val = val_df.pop('target')\n",
    "    X_val = val_df\n",
    "\n",
    "    y_pred = model.predict(X_val)\n",
    "    auc = roc_auc_score(y_val, y_pred)\n",
    "\n",
    "    report_dict = {\n",
    "        \"binary_classification_metrics\": {\n",
    "            \"auc\": {\n",
    "                \"value\": auc\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    os.makedirs(args.output_dir, exist_ok=True)\n",
    "    evaluation_path = os.path.join(args.output_dir, \"evaluation.json\")\n",
    "    with open(evaluation_path, \"w\") as f:\n",
    "        json.dump(report_dict, f)\n",
    "\"\"\"\n",
    "with open('evaluate.py', 'w') as f:\n",
    "    f.write(evaluation_script)\n",
    "\n",
    "# 評価ステップの定義\n",
    "evaluation_processor = ScriptProcessor(\n",
    "    image_uri=script_processor.image_uri,\n",
    "    command=['python3'],\n",
    "    instance_type=processing_instance_type,\n",
    "    instance_count=1,\n",
    "    base_job_name='lightgbm-evaluation',\n",
    "    role=role,\n",
    "    sagemaker_session=sagemaker_session\n",
    ")\n",
    "\n",
    "evaluation_report = PropertyFile(\n",
    "    name='EvaluationReport',\n",
    "    output_name='evaluation',\n",
    "    path='evaluation.json',\n",
    ")\n",
    "\n",
    "evaluation_step = ProcessingStep(\n",
    "    name='Evaluation',\n",
    "    processor=evaluation_processor,\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            source=training_step.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "            destination='/opt/ml/processing/model'\n",
    "        ),\n",
    "        ProcessingInput(\n",
    "            source=processing_step.properties.ProcessingOutputConfig.Outputs['validation_data'].S3Output.S3Uri,\n",
    "            destination='/opt/ml/processing/validation'\n",
    "        ),\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(\n",
    "            output_name='evaluation',\n",
    "            source='/opt/ml/processing/evaluation',\n",
    "        ),\n",
    "    ],\n",
    "    code='evaluate.py',\n",
    "    property_files=[evaluation_report],\n",
    ")\n",
    "\n",
    "# 推論スクリプトの作成\n",
    "inference_script = \"\"\"\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from io import StringIO\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    print(\"モデルを読み込んでいます。\")\n",
    "    model = lgb.Booster(model_file=os.path.join(model_dir, 'model.txt'))\n",
    "    return model\n",
    "\n",
    "def input_fn(request_body, content_type):\n",
    "    if content_type == 'text/csv':\n",
    "        return pd.read_csv(StringIO(request_body), header=None)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported content type: {}\".format(content_type))\n",
    "\n",
    "def predict_fn(input_data, model):\n",
    "    predictions = model.predict(input_data)\n",
    "    return predictions\n",
    "\n",
    "def output_fn(prediction, accept):\n",
    "    if accept == 'text/csv':\n",
    "        return ','.join(map(str, prediction.tolist()))\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported accept type: {}\".format(accept))\n",
    "\"\"\"\n",
    "with open('inference.py', 'w') as f:\n",
    "    f.write(inference_script)\n",
    "\n",
    "# モデルステップの定義（モデルの登録）\n",
    "model = Model(\n",
    "    image_uri=sklearn_estimator.training_image_uri(),\n",
    "    model_data=training_step.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    role=role,\n",
    "    entry_point='inference.py',\n",
    "    sagemaker_session=sagemaker_session,\n",
    ")\n",
    "\n",
    "model_step = ModelStep(\n",
    "    name='RegisterModel',\n",
    "    step_args=model.create(instance_type='ml.m5.large'),\n",
    ")\n",
    "\n",
    "# 条件ステップの定義\n",
    "cond_gte = ConditionGreaterThanOrEqualTo(\n",
    "    left=JsonGet(\n",
    "        step_name=evaluation_step.name,\n",
    "        property_file=evaluation_report,\n",
    "        json_path=\"binary_classification_metrics.auc.value\",\n",
    "    ),\n",
    "    right=0.8,\n",
    ")\n",
    "\n",
    "condition_step = ConditionStep(\n",
    "    name='AUCConditionCheck',\n",
    "    conditions=[cond_gte],\n",
    "    if_steps=[model_step],\n",
    "    else_steps=[],\n",
    ")\n",
    "\n",
    "# パイプラインの定義\n",
    "pipeline = Pipeline(\n",
    "    name='LightGBM-Pipeline-ModelRegistration',\n",
    "    parameters=[\n",
    "        processing_instance_type,\n",
    "        training_instance_type,\n",
    "        model_approval_status,\n",
    "    ],\n",
    "    steps=[processing_step, training_step, evaluation_step, condition_step],\n",
    "    sagemaker_session=sagemaker_session,\n",
    ")\n",
    "\n",
    "# パイプラインの作成と実行\n",
    "pipeline.upsert(role_arn=role)\n",
    "execution = pipeline.start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a76fa6b2-5324-4ade-ad89-08d9c8f65d9a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3792328313.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[8], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    pip install --upgrade sagemaker\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade sagemaker\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.session import Session\n",
    "from sagemaker.workflow.pipeline_context import PipelineSession\n",
    "from sagemaker.workflow.parameters import ParameterString\n",
    "from sagemaker.workflow.steps import ProcessingStep, TrainingStep\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "from sagemaker.processing import ScriptProcessor, ProcessingInput, ProcessingOutput\n",
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "from sagemaker.workflow.condition_step import ConditionStep\n",
    "from sagemaker.workflow.conditions import ConditionGreaterThanOrEqualTo\n",
    "from sagemaker.workflow.functions import JsonGet\n",
    "from sagemaker.workflow.model_step import RegisterModel\n",
    "from sagemaker.workflow.properties import PropertyFile\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# セッションとロールの設定\n",
    "sagemaker_session = sagemaker.session.Session()\n",
    "role = get_execution_role()\n",
    "region = sagemaker_session.boto_region_name\n",
    "default_bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "# パイプラインセッションの作成\n",
    "pipeline_session = PipelineSession()\n",
    "\n",
    "# パラメータの定義\n",
    "processing_instance_type = ParameterString(name=\"ProcessingInstanceType\", default_value=\"ml.m5.xlarge\")\n",
    "training_instance_type = ParameterString(name=\"TrainingInstanceType\", default_value=\"ml.m5.xlarge\")\n",
    "model_approval_status = ParameterString(name=\"ModelApprovalStatus\", default_value=\"PendingManualApproval\")\n",
    "\n",
    "# データのS3パスの定義\n",
    "input_data_uri = f's3://{default_bucket}/lightgbm-pipeline/input/data.csv'\n",
    "output_data_uri = f's3://{default_bucket}/lightgbm-pipeline/output'\n",
    "\n",
    "# サンプルデータの作成とS3へのアップロード\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X, y = make_classification(\n",
    "    n_samples=1000, n_features=20, n_informative=15, n_redundant=5, n_classes=2, random_state=42\n",
    ")\n",
    "df = pd.DataFrame(X)\n",
    "df['target'] = y\n",
    "\n",
    "# データをS3にアップロード\n",
    "os.makedirs('data', exist_ok=True)\n",
    "df.to_csv('data/data.csv', index=False)\n",
    "sagemaker_session.upload_data(path='data/data.csv', bucket=default_bucket, key_prefix='lightgbm-pipeline/input')\n",
    "\n",
    "# 前処理スクリプトの作成\n",
    "processing_script = \"\"\"\n",
    "import argparse\n",
    "import os\n",
    "import pandas as pd\n",
    "import boto3\n",
    "import logging\n",
    "from sklearn.model_selection import train_test_split\n",
    "from io import StringIO\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    logger.info(\"前処理を開始します。\")\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--input-bucket\", type=str)\n",
    "    parser.add_argument(\"--input-key\", type=str)\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    logger.info(\"S3からデータを読み込んでいます。\")\n",
    "    s3 = boto3.client('s3')\n",
    "    response = s3.get_object(Bucket=args.input_bucket, Key=args.input_key)\n",
    "    df = pd.read_csv(StringIO(response['Body'].read().decode('utf-8')))\n",
    "\n",
    "    logger.info(f\"入力データの形状: {df.shape}\")\n",
    "\n",
    "    logger.info(\"データを訓練セットと検証セットに分割します。\")\n",
    "    train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "    os.makedirs('/opt/ml/processing/train', exist_ok=True)\n",
    "    os.makedirs('/opt/ml/processing/validation', exist_ok=True)\n",
    "\n",
    "    logger.info(\"訓練データを保存します。\")\n",
    "    train_df.to_csv('/opt/ml/processing/train/train.csv', index=False)\n",
    "\n",
    "    logger.info(\"検証データを保存します。\")\n",
    "    val_df.to_csv('/opt/ml/processing/validation/validation.csv', index=False)\n",
    "\n",
    "    logger.info(\"前処理が完了しました。\")\n",
    "\"\"\"\n",
    "with open('preprocessing.py', 'w') as f:\n",
    "    f.write(processing_script)\n",
    "\n",
    "# 前処理ステップの定義\n",
    "script_processor = ScriptProcessor(\n",
    "    image_uri=sagemaker.image_uris.retrieve(framework='sklearn', region=region, version='0.23-1'),\n",
    "    command=['python3'],\n",
    "    instance_type=processing_instance_type,\n",
    "    instance_count=1,\n",
    "    base_job_name='lightgbm-preprocessing',\n",
    "    role=role,\n",
    "    sagemaker_session=pipeline_session  # PipelineSessionを使用\n",
    ")\n",
    "\n",
    "processing_step = ProcessingStep(\n",
    "    name='Preprocessing',\n",
    "    processor=script_processor,\n",
    "    inputs=[],\n",
    "    outputs=[\n",
    "        ProcessingOutput(\n",
    "            output_name='train_data',\n",
    "            source='/opt/ml/processing/train',\n",
    "            destination=f's3://{default_bucket}/lightgbm-pipeline/output/train'\n",
    "        ),\n",
    "        ProcessingOutput(\n",
    "            output_name='validation_data',\n",
    "            source='/opt/ml/processing/validation',\n",
    "            destination=f's3://{default_bucket}/lightgbm-pipeline/output/validation'\n",
    "        ),\n",
    "    ],\n",
    "    code='preprocessing.py',\n",
    "    job_arguments=[\n",
    "        \"--input-bucket\", default_bucket,\n",
    "        \"--input-key\", \"lightgbm-pipeline/input/data.csv\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 学習スクリプトの作成\n",
    "training_script = \"\"\"\n",
    "import argparse\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# 必要なパッケージをインストール\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"lightgbm\", \"optuna\"])\n",
    "\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import optuna\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def objective(trial, X_train, y_train, X_val, y_val):\n",
    "    params = {\n",
    "        'objective': 'binary',\n",
    "        'metric': 'auc',\n",
    "        'verbosity': -1,\n",
    "        'boosting_type': 'gbdt',\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 20, 150),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 1e-4, 1e-1),\n",
    "        'feature_fraction': trial.suggest_uniform('feature_fraction', 0.6, 1.0),\n",
    "        'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.6, 1.0),\n",
    "        'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n",
    "        'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-4, 10.0),\n",
    "        'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-4, 10.0),\n",
    "    }\n",
    "\n",
    "    train_dataset = lgb.Dataset(X_train, label=y_train)\n",
    "    val_dataset = lgb.Dataset(X_val, label=y_val)\n",
    "\n",
    "    model = lgb.train(params, train_dataset, valid_sets=[val_dataset], early_stopping_rounds=10, verbose_eval=False)\n",
    "    y_pred = model.predict(X_val)\n",
    "    auc = roc_auc_score(y_val, y_pred)\n",
    "    return auc\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--n-trials\", type=int, default=20)\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    print(\"訓練データを読み込んでいます。\")\n",
    "    train_df = pd.read_csv('/opt/ml/input/data/train/train.csv')\n",
    "\n",
    "    print(\"検証データを読み込んでいます。\")\n",
    "    val_df = pd.read_csv('/opt/ml/input/data/validation/validation.csv')\n",
    "\n",
    "    y_train = train_df.pop('target')\n",
    "    X_train = train_df\n",
    "    y_val = val_df.pop('target')\n",
    "    X_val = val_df\n",
    "\n",
    "    # Optunaによるハイパーパラメータ最適化\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(lambda trial: objective(trial, X_train, y_train, X_val, y_val), n_trials=args.n_trials)\n",
    "\n",
    "    print('Best trial:')\n",
    "    trial = study.best_trial\n",
    "    print('  AUC: {}'.format(trial.value))\n",
    "    print('  Params: ')\n",
    "    for key, value in trial.params.items():\n",
    "        print('    {}: {}'.format(key, value))\n",
    "\n",
    "    # 最適なハイパーパラメータでモデルを再学習\n",
    "    best_params = trial.params\n",
    "    best_params['objective'] = 'binary'\n",
    "    best_params['metric'] = 'auc'\n",
    "    best_params['verbosity'] = -1\n",
    "    best_params['boosting_type'] = 'gbdt'\n",
    "\n",
    "    train_dataset = lgb.Dataset(X_train, label=y_train)\n",
    "    val_dataset = lgb.Dataset(X_val, label=y_val)\n",
    "\n",
    "    model = lgb.train(best_params, train_dataset, valid_sets=[val_dataset], early_stopping_rounds=10)\n",
    "\n",
    "    # モデルを保存\n",
    "    model_dir = os.environ.get('SM_MODEL_DIR')\n",
    "    model.save_model(os.path.join(model_dir, \"model.txt\"))\n",
    "\"\"\"\n",
    "with open('train.py', 'w') as f:\n",
    "    f.write(training_script)\n",
    "\n",
    "# 学習ステップの定義\n",
    "sklearn_estimator = SKLearn(\n",
    "    entry_point='train.py',\n",
    "    role=role,\n",
    "    instance_type=training_instance_type,\n",
    "    instance_count=1,\n",
    "    framework_version='0.23-1',\n",
    "    base_job_name='lightgbm-training',\n",
    "    sagemaker_session=pipeline_session,  # PipelineSessionを使用\n",
    "    hyperparameters={\n",
    "        'n-trials': 50,  # Optunaの試行回数\n",
    "    },\n",
    ")\n",
    "\n",
    "training_step = TrainingStep(\n",
    "    name='Training',\n",
    "    estimator=sklearn_estimator,\n",
    "    inputs={\n",
    "        'train': sagemaker.inputs.TrainingInput(\n",
    "            s3_data=processing_step.properties.ProcessingOutputConfig.Outputs['train_data'].S3Output.S3Uri,\n",
    "            content_type='text/csv'\n",
    "        ),\n",
    "        'validation': sagemaker.inputs.TrainingInput(\n",
    "            s3_data=processing_step.properties.ProcessingOutputConfig.Outputs['validation_data'].S3Output.S3Uri,\n",
    "            content_type='text/csv'\n",
    "        ),\n",
    "    },\n",
    ")\n",
    "\n",
    "# 評価スクリプトの作成\n",
    "evaluation_script = \"\"\"\n",
    "import argparse\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"lightgbm\"])\n",
    "\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import json\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--output-dir\", type=str, default=\"/opt/ml/processing/evaluation\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    print(\"モデルを読み込んでいます。\")\n",
    "    model = lgb.Booster(model_file=os.path.join('/opt/ml/processing/model', 'model.txt'))\n",
    "\n",
    "    print(\"検証データを読み込んでいます。\")\n",
    "    val_df = pd.read_csv('/opt/ml/processing/validation/validation.csv')\n",
    "    y_val = val_df.pop('target')\n",
    "    X_val = val_df\n",
    "\n",
    "    y_pred = model.predict(X_val)\n",
    "    auc = roc_auc_score(y_val, y_pred)\n",
    "\n",
    "    report_dict = {\n",
    "        \"binary_classification_metrics\": {\n",
    "            \"auc\": {\n",
    "                \"value\": auc\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    os.makedirs(args.output_dir, exist_ok=True)\n",
    "    evaluation_path = os.path.join(args.output_dir, \"evaluation.json\")\n",
    "    with open(evaluation_path, \"w\") as f:\n",
    "        json.dump(report_dict, f)\n",
    "\"\"\"\n",
    "with open('evaluate.py', 'w') as f:\n",
    "    f.write(evaluation_script)\n",
    "\n",
    "# 評価ステップの定義\n",
    "evaluation_processor = ScriptProcessor(\n",
    "    image_uri=script_processor.image_uri,\n",
    "    command=['python3'],\n",
    "    instance_type=processing_instance_type,\n",
    "    instance_count=1,\n",
    "    base_job_name='lightgbm-evaluation',\n",
    "    role=role,\n",
    "    sagemaker_session=pipeline_session  # PipelineSessionを使用\n",
    ")\n",
    "\n",
    "evaluation_report = PropertyFile(\n",
    "    name='EvaluationReport',\n",
    "    output_name='evaluation',\n",
    "    path='evaluation.json',\n",
    ")\n",
    "\n",
    "evaluation_step = ProcessingStep(\n",
    "    name='Evaluation',\n",
    "    processor=evaluation_processor,\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            source=training_step.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "            destination='/opt/ml/processing/model'\n",
    "        ),\n",
    "        ProcessingInput(\n",
    "            source=processing_step.properties.ProcessingOutputConfig.Outputs['validation_data'].S3Output.S3Uri,\n",
    "            destination='/opt/ml/processing/validation'\n",
    "        ),\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(\n",
    "            output_name='evaluation',\n",
    "            source='/opt/ml/processing/evaluation',\n",
    "        ),\n",
    "    ],\n",
    "    code='evaluate.py',\n",
    "    property_files=[evaluation_report],\n",
    ")\n",
    "\n",
    "# モデルパッケージグループ名の定義\n",
    "model_package_group_name = 'LightGBMModelPackageGroup'\n",
    "\n",
    "# モデル登録ステップの定義\n",
    "register_model_step = RegisterModel(\n",
    "    name='RegisterModel',\n",
    "    estimator=sklearn_estimator,\n",
    "    model_data=training_step.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    content_types=['text/csv'],\n",
    "    response_types=['text/csv'],\n",
    "    inference_instances=['ml.m5.large'],\n",
    "    transform_instances=['ml.m5.large'],\n",
    "    model_package_group_name=model_package_group_name,\n",
    "    approval_status=model_approval_status,\n",
    "    model_metrics={\n",
    "        \"ModelQuality\": {\n",
    "            \"Statistics\": {\n",
    "                \"ContentType\": \"application/json\",\n",
    "                \"S3Uri\": evaluation_step.properties.ProcessingOutputConfig.Outputs[\"evaluation\"].S3Output.S3Uri\n",
    "            }\n",
    "        }\n",
    "    },\n",
    ")\n",
    "\n",
    "# 条件ステップの定義\n",
    "cond_gte = ConditionGreaterThanOrEqualTo(\n",
    "    left=JsonGet(\n",
    "        step_name=evaluation_step.name,\n",
    "        property_file=evaluation_report,\n",
    "        json_path=\"binary_classification_metrics.auc.value\",\n",
    "    ),\n",
    "    right=0.8,\n",
    ")\n",
    "\n",
    "condition_step = ConditionStep(\n",
    "    name='AUCConditionCheck',\n",
    "    conditions=[cond_gte],\n",
    "    if_steps=[register_model_step],\n",
    "    else_steps=[],\n",
    ")\n",
    "\n",
    "# パイプラインの定義\n",
    "pipeline = Pipeline(\n",
    "    name='LightGBM-Pipeline-ModelRegistration',\n",
    "    parameters=[\n",
    "        processing_instance_type,\n",
    "        training_instance_type,\n",
    "        model_approval_status,\n",
    "    ],\n",
    "    steps=[processing_step, training_step, evaluation_step, condition_step],\n",
    "    sagemaker_session=pipeline_session,  # PipelineSessionを使用\n",
    ")\n",
    "\n",
    "# パイプラインの作成と実行\n",
    "pipeline.upsert(role_arn=role)\n",
    "execution = pipeline.start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cdc2fabd-2d6a-48de-a075-59833ba10ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.model_step import ModelStep\n",
    "from sagemaker.model import Model\n",
    "from sagemaker.serverless import ServerlessInferenceConfig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2e20729-7fd4-48c1-ad75-7fdd31a52f79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sagemaker in /opt/conda/lib/python3.10/site-packages (2.232.1)\n",
      "Requirement already satisfied: attrs<24,>=23.1.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (23.2.0)\n",
      "Requirement already satisfied: boto3<2.0,>=1.34.142 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (1.35.24)\n",
      "Requirement already satisfied: cloudpickle==2.2.1 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (2.2.1)\n",
      "Requirement already satisfied: docker in /opt/conda/lib/python3.10/site-packages (from sagemaker) (7.1.0)\n",
      "Requirement already satisfied: google-pasta in /opt/conda/lib/python3.10/site-packages (from sagemaker) (0.2.0)\n",
      "Requirement already satisfied: importlib-metadata<7.0,>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (6.10.0)\n",
      "Requirement already satisfied: jsonschema in /opt/conda/lib/python3.10/site-packages (from sagemaker) (4.17.3)\n",
      "Requirement already satisfied: numpy<2.0,>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (23.2)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from sagemaker) (2.1.4)\n",
      "Requirement already satisfied: pathos in /opt/conda/lib/python3.10/site-packages (from sagemaker) (0.3.2)\n",
      "Requirement already satisfied: platformdirs in /opt/conda/lib/python3.10/site-packages (from sagemaker) (4.2.2)\n",
      "Requirement already satisfied: protobuf<5.0,>=3.12 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (4.24.4)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from sagemaker) (5.9.8)\n",
      "Requirement already satisfied: pyyaml~=6.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (6.0.1)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from sagemaker) (2.32.3)\n",
      "Requirement already satisfied: sagemaker-core<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (1.0.6)\n",
      "Requirement already satisfied: schema in /opt/conda/lib/python3.10/site-packages (from sagemaker) (0.7.7)\n",
      "Requirement already satisfied: smdebug-rulesconfig==1.0.1 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (1.0.1)\n",
      "Requirement already satisfied: tblib<4,>=1.7.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (2.0.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from sagemaker) (4.66.4)\n",
      "Requirement already satisfied: urllib3<3.0.0,>=1.26.8 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (1.26.19)\n",
      "Requirement already satisfied: botocore<1.36.0,>=1.35.24 in /opt/conda/lib/python3.10/site-packages (from boto3<2.0,>=1.34.142->sagemaker) (1.35.24)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from boto3<2.0,>=1.34.142->sagemaker) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /opt/conda/lib/python3.10/site-packages (from boto3<2.0,>=1.34.142->sagemaker) (0.10.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata<7.0,>=1.4.0->sagemaker) (3.19.2)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=1.7.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker-core<2.0.0,>=1.0.0->sagemaker) (1.10.16)\n",
      "Requirement already satisfied: rich<14.0.0,>=13.0.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker-core<2.0.0,>=1.0.0->sagemaker) (13.7.1)\n",
      "Requirement already satisfied: mock<5.0,>4.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker-core<2.0.0,>=1.0.0->sagemaker) (4.0.3)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from jsonschema->sagemaker) (0.20.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->sagemaker) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->sagemaker) (3.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->sagemaker) (2024.7.4)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from google-pasta->sagemaker) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->sagemaker) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->sagemaker) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->sagemaker) (2024.1)\n",
      "Requirement already satisfied: ppft>=1.7.6.8 in /opt/conda/lib/python3.10/site-packages (from pathos->sagemaker) (1.7.6.8)\n",
      "Requirement already satisfied: dill>=0.3.8 in /opt/conda/lib/python3.10/site-packages (from pathos->sagemaker) (0.3.8)\n",
      "Requirement already satisfied: pox>=0.3.4 in /opt/conda/lib/python3.10/site-packages (from pathos->sagemaker) (0.3.4)\n",
      "Requirement already satisfied: multiprocess>=0.70.16 in /opt/conda/lib/python3.10/site-packages (from pathos->sagemaker) (0.70.16)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3.0.0,>=1.7.0->sagemaker-core<2.0.0,>=1.0.0->sagemaker) (4.12.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich<14.0.0,>=13.0.0->sagemaker-core<2.0.0,>=1.0.0->sagemaker) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich<14.0.0,>=13.0.0->sagemaker-core<2.0.0,>=1.0.0->sagemaker) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=13.0.0->sagemaker-core<2.0.0,>=1.0.0->sagemaker) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ec6ef76c-e35e-41ed-9ca2-3f6c37749f4e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Defaulting to only available Python version: py3\n",
      "INFO:sagemaker.image_uris:Defaulting to only supported image scope: cpu.\n",
      "WARNING:sagemaker:instance_type is a PipelineVariable (<class 'sagemaker.workflow.parameters.ParameterString'>). Its interpreted value in execution time should not be of GPU types since GPU training is not supported for Scikit-Learn.\n",
      "WARNING:sagemaker.workflow.utilities:The input argument instance_type of function (sagemaker.image_uris.retrieve) is a pipeline variable (<class 'sagemaker.workflow.parameters.ParameterString'>), which is interpreted in pipeline execution time only. As the function needs to evaluate the argument value in SDK compile time, the default_value of this Parameter object will be used to override it. Please make sure the default_value is valid.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:The input argument instance_type of function (sagemaker.image_uris.get_training_image_uri) is a pipeline variable (<class 'sagemaker.workflow.parameters.ParameterString'>), which is interpreted in pipeline execution time only. As the function needs to evaluate the argument value in SDK compile time, the default_value of this Parameter object will be used to override it. Please make sure the default_value is valid.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TrainingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:The input argument instance_type of function (sagemaker.image_uris.get_training_image_uri) is a pipeline variable (<class 'sagemaker.workflow.parameters.ParameterString'>), which is interpreted in pipeline execution time only. As the function needs to evaluate the argument value in SDK compile time, the default_value of this Parameter object will be used to override it. Please make sure the default_value is valid.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TrainingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.session import Session\n",
    "from sagemaker.workflow.pipeline_context import PipelineSession\n",
    "from sagemaker.workflow.parameters import ParameterString\n",
    "from sagemaker.workflow.steps import ProcessingStep, TrainingStep\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "from sagemaker.processing import ScriptProcessor, ProcessingInput, ProcessingOutput\n",
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "from sagemaker.workflow.condition_step import ConditionStep\n",
    "from sagemaker.workflow.conditions import ConditionGreaterThanOrEqualTo\n",
    "from sagemaker.workflow.functions import JsonGet\n",
    "# from sagemaker.workflow.model_step import RegisterModel\n",
    "from sagemaker.workflow.properties import PropertyFile\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# セッションとロールの設定\n",
    "sagemaker_session = sagemaker.session.Session()\n",
    "role = get_execution_role()\n",
    "region = sagemaker_session.boto_region_name\n",
    "default_bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "# パイプラインセッションの作成\n",
    "pipeline_session = PipelineSession()\n",
    "\n",
    "# パラメータの定義\n",
    "processing_instance_type = ParameterString(name=\"ProcessingInstanceType\", default_value=\"ml.m5.xlarge\")\n",
    "training_instance_type = ParameterString(name=\"TrainingInstanceType\", default_value=\"ml.m5.xlarge\")\n",
    "model_approval_status = ParameterString(name=\"ModelApprovalStatus\", default_value=\"PendingManualApproval\")\n",
    "\n",
    "# データのS3パスの定義\n",
    "input_data_uri = f's3://{default_bucket}/lightgbm-pipeline/input/data.csv'\n",
    "output_data_uri = f's3://{default_bucket}/lightgbm-pipeline/output'\n",
    "\n",
    "# サンプルデータの作成とS3へのアップロード\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X, y = make_classification(\n",
    "    n_samples=1000, n_features=20, n_informative=15, n_redundant=5, n_classes=2, random_state=42\n",
    ")\n",
    "df = pd.DataFrame(X)\n",
    "df['target'] = y\n",
    "\n",
    "# データをS3にアップロード\n",
    "os.makedirs('data', exist_ok=True)\n",
    "df.to_csv('data/data.csv', index=False)\n",
    "sagemaker_session.upload_data(path='data/data.csv', bucket=default_bucket, key_prefix='lightgbm-pipeline/input')\n",
    "\n",
    "# 前処理スクリプトの作成\n",
    "processing_script = \"\"\"\n",
    "import argparse\n",
    "import os\n",
    "import pandas as pd\n",
    "import boto3\n",
    "import logging\n",
    "from sklearn.model_selection import train_test_split\n",
    "from io import StringIO\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    logger.info(\"前処理を開始します。\")\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--input-bucket\", type=str)\n",
    "    parser.add_argument(\"--input-key\", type=str)\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    logger.info(\"S3からデータを読み込んでいます。\")\n",
    "    s3 = boto3.client('s3')\n",
    "    response = s3.get_object(Bucket=args.input_bucket, Key=args.input_key)\n",
    "    df = pd.read_csv(StringIO(response['Body'].read().decode('utf-8')))\n",
    "\n",
    "    logger.info(f\"入力データの形状: {df.shape}\")\n",
    "\n",
    "    logger.info(\"データを訓練セットと検証セットに分割します。\")\n",
    "    train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "    os.makedirs('/opt/ml/processing/train', exist_ok=True)\n",
    "    os.makedirs('/opt/ml/processing/validation', exist_ok=True)\n",
    "\n",
    "    logger.info(\"訓練データを保存します。\")\n",
    "    train_df.to_csv('/opt/ml/processing/train/train.csv', index=False)\n",
    "\n",
    "    logger.info(\"検証データを保存します。\")\n",
    "    val_df.to_csv('/opt/ml/processing/validation/validation.csv', index=False)\n",
    "\n",
    "    logger.info(\"前処理が完了しました。\")\n",
    "\"\"\"\n",
    "with open('preprocessing.py', 'w') as f:\n",
    "    f.write(processing_script)\n",
    "\n",
    "# 前処理ステップの定義\n",
    "script_processor = ScriptProcessor(\n",
    "    image_uri=sagemaker.image_uris.retrieve(framework='sklearn', region=region, version='0.23-1'),\n",
    "    command=['python3'],\n",
    "    instance_type=processing_instance_type,\n",
    "    instance_count=1,\n",
    "    base_job_name='lightgbm-preprocessing',\n",
    "    role=role,\n",
    "    sagemaker_session=pipeline_session  # PipelineSessionを使用\n",
    ")\n",
    "\n",
    "processing_step = ProcessingStep(\n",
    "    name='Preprocessing',\n",
    "    processor=script_processor,\n",
    "    inputs=[],\n",
    "    outputs=[\n",
    "        ProcessingOutput(\n",
    "            output_name='train_data',\n",
    "            source='/opt/ml/processing/train',\n",
    "            destination=f's3://{default_bucket}/lightgbm-pipeline/output/train'\n",
    "        ),\n",
    "        ProcessingOutput(\n",
    "            output_name='validation_data',\n",
    "            source='/opt/ml/processing/validation',\n",
    "            destination=f's3://{default_bucket}/lightgbm-pipeline/output/validation'\n",
    "        ),\n",
    "    ],\n",
    "    code='preprocessing.py',\n",
    "    job_arguments=[\n",
    "        \"--input-bucket\", default_bucket,\n",
    "        \"--input-key\", \"lightgbm-pipeline/input/data.csv\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 学習スクリプトの作成\n",
    "training_script = \"\"\"\n",
    "import argparse\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# 必要なパッケージをインストール\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"lightgbm\", \"optuna\"])\n",
    "\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import optuna\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def objective(trial, X_train, y_train, X_val, y_val):\n",
    "    params = {\n",
    "        'objective': 'binary',\n",
    "        'metric': 'auc',\n",
    "        'verbosity': -1,\n",
    "        'boosting_type': 'gbdt',\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 20, 150),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 1e-4, 1e-1),\n",
    "        'feature_fraction': trial.suggest_uniform('feature_fraction', 0.6, 1.0),\n",
    "        'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.6, 1.0),\n",
    "        'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n",
    "        'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-4, 10.0),\n",
    "        'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-4, 10.0),\n",
    "    }\n",
    "\n",
    "    train_dataset = lgb.Dataset(X_train, label=y_train)\n",
    "    val_dataset = lgb.Dataset(X_val, label=y_val)\n",
    "\n",
    "    model = lgb.train(params, train_dataset, valid_sets=[val_dataset],)\n",
    "    y_pred = model.predict(X_val)\n",
    "    auc = roc_auc_score(y_val, y_pred)\n",
    "    return auc\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--n-trials\", type=int, default=20)\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    print(\"訓練データを読み込んでいます。\")\n",
    "    train_df = pd.read_csv('/opt/ml/input/data/train/train.csv')\n",
    "\n",
    "    print(\"検証データを読み込んでいます。\")\n",
    "    val_df = pd.read_csv('/opt/ml/input/data/validation/validation.csv')\n",
    "\n",
    "    y_train = train_df.pop('target')\n",
    "    X_train = train_df\n",
    "    y_val = val_df.pop('target')\n",
    "    X_val = val_df\n",
    "\n",
    "    # Optunaによるハイパーパラメータ最適化\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(lambda trial: objective(trial, X_train, y_train, X_val, y_val), \n",
    "    n_trials=args.n_trials)\n",
    "\n",
    "    print('Best trial:')\n",
    "    trial = study.best_trial\n",
    "    print('  AUC: {}'.format(trial.value))\n",
    "    print('  Params: ')\n",
    "    for key, value in trial.params.items():\n",
    "        print('    {}: {}'.format(key, value))\n",
    "\n",
    "    # 最適なハイパーパラメータでモデルを再学習\n",
    "    best_params = trial.params\n",
    "    best_params['objective'] = 'binary'\n",
    "    best_params['metric'] = 'auc'\n",
    "    best_params['verbosity'] = -1\n",
    "    best_params['boosting_type'] = 'gbdt'\n",
    "\n",
    "    train_dataset = lgb.Dataset(X_train, label=y_train)\n",
    "    val_dataset = lgb.Dataset(X_val, label=y_val)\n",
    "\n",
    "    model = lgb.train(best_params, train_dataset, valid_sets=[val_dataset], )\n",
    "\n",
    "    # モデルを保存\n",
    "    model_dir = os.environ.get('SM_MODEL_DIR')\n",
    "    model.save_model(os.path.join(model_dir, \"model.txt\"))\n",
    "\"\"\"\n",
    "with open('train.py', 'w') as f:\n",
    "    f.write(training_script)\n",
    "\n",
    "# 学習ステップの定義\n",
    "sklearn_estimator = SKLearn(\n",
    "    entry_point='train.py',\n",
    "    role=role,\n",
    "    instance_type=training_instance_type,\n",
    "    instance_count=1,\n",
    "    framework_version='0.23-1',\n",
    "    base_job_name='lightgbm-training',\n",
    "    sagemaker_session=pipeline_session,  # PipelineSessionを使用\n",
    "    hyperparameters={\n",
    "        'n-trials': 50,  # Optunaの試行回数\n",
    "    },\n",
    ")\n",
    "\n",
    "training_step = TrainingStep(\n",
    "    name='Training',\n",
    "    estimator=sklearn_estimator,\n",
    "    inputs={\n",
    "        'train': sagemaker.inputs.TrainingInput(\n",
    "            s3_data=processing_step.properties.ProcessingOutputConfig.Outputs['train_data'].S3Output.S3Uri,\n",
    "            content_type='text/csv'\n",
    "        ),\n",
    "        'validation': sagemaker.inputs.TrainingInput(\n",
    "            s3_data=processing_step.properties.ProcessingOutputConfig.Outputs['validation_data'].S3Output.S3Uri,\n",
    "            content_type='text/csv'\n",
    "        ),\n",
    "    },\n",
    ")\n",
    "\n",
    "# 評価スクリプトの作成\n",
    "evaluation_script = \"\"\"\n",
    "import argparse\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"lightgbm\"])\n",
    "\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import json\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--output-dir\", type=str, default=\"/opt/ml/processing/evaluation\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    print(\"モデルを読み込んでいます。\")\n",
    "    model = lgb.Booster(model_file=os.path.join('/opt/ml/processing/model', 'model.txt'))\n",
    "\n",
    "    print(\"検証データを読み込んでいます。\")\n",
    "    val_df = pd.read_csv('/opt/ml/processing/validation/validation.csv')\n",
    "    y_val = val_df.pop('target')\n",
    "    X_val = val_df\n",
    "\n",
    "    y_pred = model.predict(X_val)\n",
    "    auc = roc_auc_score(y_val, y_pred)\n",
    "\n",
    "    report_dict = {\n",
    "        \"binary_classification_metrics\": {\n",
    "            \"auc\": {\n",
    "                \"value\": auc\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    os.makedirs(args.output_dir, exist_ok=True)\n",
    "    evaluation_path = os.path.join(args.output_dir, \"evaluation.json\")\n",
    "    with open(evaluation_path, \"w\") as f:\n",
    "        json.dump(report_dict, f)\n",
    "\"\"\"\n",
    "with open('evaluate.py', 'w') as f:\n",
    "    f.write(evaluation_script)\n",
    "\n",
    "# 評価ステップの定義\n",
    "evaluation_processor = ScriptProcessor(\n",
    "    image_uri=script_processor.image_uri,\n",
    "    command=['python3'],\n",
    "    instance_type=processing_instance_type,\n",
    "    instance_count=1,\n",
    "    base_job_name='lightgbm-evaluation',\n",
    "    role=role,\n",
    "    sagemaker_session=pipeline_session  # PipelineSessionを使用\n",
    ")\n",
    "\n",
    "evaluation_report = PropertyFile(\n",
    "    name='EvaluationReport',\n",
    "    output_name='evaluation',\n",
    "    path='evaluation.json',\n",
    ")\n",
    "\n",
    "evaluation_step = ProcessingStep(\n",
    "    name='Evaluation',\n",
    "    processor=evaluation_processor,\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            source=training_step.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "            destination='/opt/ml/processing/model'\n",
    "        ),\n",
    "        ProcessingInput(\n",
    "            source=processing_step.properties.ProcessingOutputConfig.Outputs['validation_data'].S3Output.S3Uri,\n",
    "            destination='/opt/ml/processing/validation'\n",
    "        ),\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(\n",
    "            output_name='evaluation',\n",
    "            source='/opt/ml/processing/evaluation',\n",
    "        ),\n",
    "    ],\n",
    "    code='evaluate.py',\n",
    "    property_files=[evaluation_report],\n",
    ")\n",
    "\n",
    "# パイプラインの定義\n",
    "pipeline = Pipeline(\n",
    "    name='LightGBM-Pipeline-ModelRegistration',\n",
    "    parameters=[\n",
    "        processing_instance_type,\n",
    "        training_instance_type,\n",
    "        model_approval_status,\n",
    "    ],\n",
    "    steps=[processing_step, \n",
    "           training_step, \n",
    "           evaluation_step, \n",
    "          ],\n",
    "    sagemaker_session=pipeline_session,  # PipelineSessionを使用\n",
    ")\n",
    "\n",
    "# パイプラインの作成と実行\n",
    "pipeline.upsert(role_arn=role)\n",
    "execution = pipeline.start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "614c1cc2-4f70-4ce4-bc62-51a3e41c3f8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker.workflow.utilities:The input argument instance_type of function (sagemaker.image_uris.get_training_image_uri) is a pipeline variable (<class 'sagemaker.workflow.parameters.ParameterString'>), which is interpreted in pipeline execution time only. As the function needs to evaluate the argument value in SDK compile time, the default_value of this Parameter object will be used to override it. Please make sure the default_value is valid.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Properties' object has no attribute 'decode'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 16\u001b[0m\n\u001b[1;32m      7\u001b[0m model \u001b[38;5;241m=\u001b[39m Model(\n\u001b[1;32m      8\u001b[0m     image_uri\u001b[38;5;241m=\u001b[39msklearn_estimator\u001b[38;5;241m.\u001b[39mtraining_image_uri(),  \u001b[38;5;66;03m# トレーニング時に使ったイメージURI\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     model_data\u001b[38;5;241m=\u001b[39mmodel_artifact_uri,                     \u001b[38;5;66;03m# モデルアーティファクトのS3パス\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     role\u001b[38;5;241m=\u001b[39mrole,\n\u001b[1;32m     11\u001b[0m     sagemaker_session\u001b[38;5;241m=\u001b[39msagemaker_session\n\u001b[1;32m     12\u001b[0m )\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# リアルタイム推論エンドポイントのデプロイ\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m predictor \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeploy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitial_instance_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;66;43;03m# エンドポイントで使用するインスタンス数\u001b[39;49;00m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43minstance_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mml.m5.large\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;66;43;03m# エンドポイントのインスタンスタイプ\u001b[39;49;00m\n\u001b[1;32m     19\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker/model.py:1698\u001b[0m, in \u001b[0;36mModel.deploy\u001b[0;34m(self, initial_instance_count, instance_type, serializer, deserializer, accelerator_type, endpoint_name, tags, kms_key, wait, data_capture_config, async_inference_config, serverless_inference_config, volume_size, model_data_download_timeout, container_startup_health_check_timeout, inference_recommendation_id, explainer_config, accept_eula, endpoint_logging, resources, endpoint_type, managed_instance_scaling, inference_component_name, routing_config, model_reference_arn, **kwargs)\u001b[0m\n\u001b[1;32m   1695\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1697\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# existing single model endpoint path\u001b[39;00m\n\u001b[0;32m-> 1698\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_sagemaker_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1699\u001b[0m \u001b[43m        \u001b[49m\u001b[43minstance_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minstance_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1700\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccelerator_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccelerator_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1701\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1702\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserverless_inference_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserverless_inference_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1703\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccept_eula\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_eula\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1704\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_reference_arn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_reference_arn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1705\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1706\u001b[0m     serverless_inference_config_dict \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1707\u001b[0m         serverless_inference_config\u001b[38;5;241m.\u001b[39m_to_request_dict() \u001b[38;5;28;01mif\u001b[39;00m is_serverless \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1708\u001b[0m     )\n\u001b[1;32m   1709\u001b[0m     production_variant \u001b[38;5;241m=\u001b[39m sagemaker\u001b[38;5;241m.\u001b[39mproduction_variant(\n\u001b[1;32m   1710\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[1;32m   1711\u001b[0m         instance_type,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1718\u001b[0m         routing_config\u001b[38;5;241m=\u001b[39mrouting_config,\n\u001b[1;32m   1719\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker/model.py:942\u001b[0m, in \u001b[0;36mModel._create_sagemaker_model\u001b[0;34m(self, instance_type, accelerator_type, tags, serverless_inference_config, accept_eula, model_reference_arn)\u001b[0m\n\u001b[1;32m    930\u001b[0m container_def \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_container_def(\n\u001b[1;32m    931\u001b[0m     instance_type,\n\u001b[1;32m    932\u001b[0m     accelerator_type\u001b[38;5;241m=\u001b[39maccelerator_type,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    935\u001b[0m     model_reference_arn\u001b[38;5;241m=\u001b[39mmodel_reference_arn,\n\u001b[1;32m    936\u001b[0m )\n\u001b[1;32m    938\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msagemaker_session, PipelineSession):\n\u001b[1;32m    939\u001b[0m     \u001b[38;5;66;03m# _base_name, model_name are not needed under PipelineSession.\u001b[39;00m\n\u001b[1;32m    940\u001b[0m     \u001b[38;5;66;03m# the model_data may be Pipeline variable\u001b[39;00m\n\u001b[1;32m    941\u001b[0m     \u001b[38;5;66;03m# which may break the _base_name generation\u001b[39;00m\n\u001b[0;32m--> 942\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_base_name_if_needed\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    943\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimage_uri\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontainer_def\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mImage\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    944\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscript_uri\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msource_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    945\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_uri\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_model_uri\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    947\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_model_name_if_needed()\n\u001b[1;32m    949\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_sagemaker_session_if_does_not_exist(instance_type)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker/model.py:998\u001b[0m, in \u001b[0;36mModel._ensure_base_name_if_needed\u001b[0;34m(self, image_uri, script_uri, model_uri)\u001b[0m\n\u001b[1;32m    991\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Create a base name from the image URI if there is no model name provided.\u001b[39;00m\n\u001b[1;32m    992\u001b[0m \n\u001b[1;32m    993\u001b[0m \u001b[38;5;124;03mIf a JumpStart script or model uri is used, select the JumpStart base name.\u001b[39;00m\n\u001b[1;32m    994\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    995\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    996\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_base_name \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    997\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_base_name\n\u001b[0;32m--> 998\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mget_jumpstart_base_name_if_jumpstart_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscript_uri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_uri\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    999\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m utils\u001b[38;5;241m.\u001b[39mbase_name_from_image(image_uri, default_base_name\u001b[38;5;241m=\u001b[39mModel\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m   1000\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker/jumpstart/utils.py:383\u001b[0m, in \u001b[0;36mget_jumpstart_base_name_if_jumpstart_model\u001b[0;34m(*uris)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return default JumpStart base name if a URI belongs to JumpStart.\u001b[39;00m\n\u001b[1;32m    376\u001b[0m \n\u001b[1;32m    377\u001b[0m \u001b[38;5;124;03mIf no URIs belong to JumpStart, return None.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;124;03m    *uris (Optional[str]): URI to test for association with JumpStart.\u001b[39;00m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m uri \u001b[38;5;129;01min\u001b[39;00m uris:\n\u001b[0;32m--> 383\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mis_jumpstart_model_uri\u001b[49m\u001b[43m(\u001b[49m\u001b[43muri\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    384\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m constants\u001b[38;5;241m.\u001b[39mJUMPSTART_RESOURCE_BASE_NAME\n\u001b[1;32m    385\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker/jumpstart/utils.py:292\u001b[0m, in \u001b[0;36mis_jumpstart_model_uri\u001b[0;34m(uri)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns True if URI corresponds to a JumpStart-hosted model.\u001b[39;00m\n\u001b[1;32m    286\u001b[0m \n\u001b[1;32m    287\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;124;03m    uri (Optional[str]): uri for inference/training job.\u001b[39;00m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    291\u001b[0m bucket \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 292\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43murlparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43muri\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mscheme \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ms3\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    293\u001b[0m     bucket, _ \u001b[38;5;241m=\u001b[39m parse_s3_url(uri)\n\u001b[1;32m    295\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m bucket \u001b[38;5;129;01min\u001b[39;00m constants\u001b[38;5;241m.\u001b[39mJUMPSTART_GATED_AND_PUBLIC_BUCKET_NAME_SET\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/urllib/parse.py:399\u001b[0m, in \u001b[0;36murlparse\u001b[0;34m(url, scheme, allow_fragments)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21murlparse\u001b[39m(url, scheme\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, allow_fragments\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    380\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Parse a URL into 6 components:\u001b[39;00m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;124;03m    <scheme>://<netloc>/<path>;<params>?<query>#<fragment>\u001b[39;00m\n\u001b[1;32m    382\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;124;03m    Note that % escapes are not expanded.\u001b[39;00m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 399\u001b[0m     url, scheme, _coerce_result \u001b[38;5;241m=\u001b[39m \u001b[43m_coerce_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheme\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    400\u001b[0m     splitresult \u001b[38;5;241m=\u001b[39m urlsplit(url, scheme, allow_fragments)\n\u001b[1;32m    401\u001b[0m     scheme, netloc, url, query, fragment \u001b[38;5;241m=\u001b[39m splitresult\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/urllib/parse.py:136\u001b[0m, in \u001b[0;36m_coerce_args\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m str_input:\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m args \u001b[38;5;241m+\u001b[39m (_noop,)\n\u001b[0;32m--> 136\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_decode_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m (_encode_result,)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/urllib/parse.py:120\u001b[0m, in \u001b[0;36m_decode_args\u001b[0;34m(args, encoding, errors)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_decode_args\u001b[39m(args, encoding\u001b[38;5;241m=\u001b[39m_implicit_encoding,\n\u001b[1;32m    119\u001b[0m                        errors\u001b[38;5;241m=\u001b[39m_implicit_errors):\n\u001b[0;32m--> 120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/urllib/parse.py:120\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_decode_args\u001b[39m(args, encoding\u001b[38;5;241m=\u001b[39m_implicit_encoding,\n\u001b[1;32m    119\u001b[0m                        errors\u001b[38;5;241m=\u001b[39m_implicit_errors):\n\u001b[0;32m--> 120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m(encoding, errors) \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m args)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Properties' object has no attribute 'decode'"
     ]
    }
   ],
   "source": [
    "from sagemaker.model import Model\n",
    "\n",
    "# 学習ステップで生成されたモデルアーティファクトを取得\n",
    "model_artifact_uri = training_step.properties.ModelArtifacts.S3ModelArtifacts\n",
    "\n",
    "# SageMaker Modelオブジェクトの作成\n",
    "model = Model(\n",
    "    image_uri=sklearn_estimator.training_image_uri(),  # トレーニングに使用したイメージURI\n",
    "    model_data=model_artifact_uri,  # トレーニング済みモデルのS3パス\n",
    "    role=role,\n",
    "    sagemaker_session=sagemaker_session\n",
    ")\n",
    "\n",
    "# リアルタイム推論エンドポイントのデプロイ\n",
    "predictor = model.deploy(\n",
    "    initial_instance_count=1,      # エンドポイントで使用するインスタンス数\n",
    "    instance_type=\"ml.m5.large\",   # エンドポイントのインスタンスタイプ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950ae5f1-9b38-4446-bb97-38edd03a0c9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b066044-515a-4155-9493-964eb1e7e68d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
