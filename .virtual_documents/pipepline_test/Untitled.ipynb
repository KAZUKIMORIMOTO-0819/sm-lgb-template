


import pandas as pd
import numpy as np
import boto3
import sagemaker
from sagemaker.session import Session
from sagemaker.workflow.pipeline import Pipeline
from sagemaker.workflow.steps import ProcessingStep, TrainingStep
from sagemaker.processing import ScriptProcessor
from sagemaker.sklearn.processing import SKLearnProcessor
from sagemaker.workflow.pipeline_context import PipelineSession
from sagemaker.inputs import TrainingInput
from sagemaker.workflow.step_collections import RegisterModel
from sagemaker.sklearn.estimator import SKLearn
from sagemaker.model_metrics import MetricsSource, ModelMetrics
from sagemaker.workflow.parameters import ParameterString

from sagemaker.sklearn.processing import SKLearnProcessor
from sagemaker.workflow.steps import ProcessingStep
from sagemaker.sklearn.estimator import SKLearn
from sagemaker.inputs import TrainingInput
from sagemaker.workflow.steps import TrainingStep

from sagemaker.workflow.pipeline import Pipeline


import os
import logging

# ログの設定
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)





from sklearn.datasets import make_classification

logger.info("サンプルデータを生成しています。")
# サンプルデータの生成
X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)
data = pd.DataFrame(X)
data['label'] = y

# データの保存
data_file = 'data.csv'
data.to_csv(data_file, index=False)
logger.info(f"サンプルデータを {data_file} に保存しました。")








# SageMakerセッションの作成
logger.info("SageMakerセッションを作成しています。")
session = sagemaker.Session()
region = session.boto_region_name
bucket = session.default_bucket()
role = sagemaker.get_execution_role()
s3_prefix = 'lightgbm-pipeline'

# パイプラインセッションの作成
pipeline_session = PipelineSession()





# データをS3にアップロード
logger.info("データをS3にアップロードしています。")
s3_uri = session.upload_data(path=data_file, bucket=bucket, key_prefix=f'{s3_prefix}/data')
logger.info(f"データが {s3_uri} にアップロードされました。")








### SKLearnProcessorを使用して前処理ステップを設定
logger.info("SKLearnProcessorを設定しています。")
sklearn_processor = SKLearnProcessor(
    framework_version='0.23-1',
    role=role,
    instance_type='ml.m5.xlarge',
    instance_count=1,
    base_job_name='lightgbm-preprocessing',
    sagemaker_session=pipeline_session
)

### ProessingStepを定義
logger.info("ProcessingStepを定義しています。")
processing_step = ProcessingStep(
    name='PreprocessingStep',
    processor=sklearn_processor,
    inputs=[
        sagemaker.processing.ProcessingInput(
            source=s3_uri, # 入力データのS3パスをs3_uriから取得する。
            destination='/opt/ml/processing/input/', # Processingジョブ内でのデータのディレクトリを指定。
            input_name='input-data' # この入力を"input_data"という名前で参照する。
        )
    ],
    
    outputs=[
        sagemaker.processing.ProcessingOutput(
            source='/opt/ml/processing/train',# Processingジョブ内での出力データの場所
            destination=f's3://{bucket}/{s3_prefix}/processing/train', # 出力データを保存するS3のパス
            output_name='train-data' # 出力データの名前
        ),
        sagemaker.processing.ProcessingOutput(
            source='/opt/ml/processing/test',
            destination=f's3://{bucket}/{s3_prefix}/processing/test',
            output_name='test-data'
        ),
    ],
    code='preprocessing.py',
)





logger.info("SKLearn Estimatorを設定しています。")
estimator = SKLearn(
    entry_point='train.py',
    role=role,
    instance_type='ml.m5.xlarge',
    instance_count=1,
    framework_version='0.23-1', # 使用するscikite-learnのバージョン指定
    py_version='py3',
    source_dir='.', # エントリーポイントスクリプトとその依存関係が含まれるディレクトリ
    dependencies=['requirements.txt'],
    output_path=f's3://{bucket}/{s3_prefix}/training_output', # トレーニングの結果を保存するS3のパス
    base_job_name='lightgbm-training',　# トレーニングジョブのベース名。ジョブの識別に使用 
    sagemaker_session=pipeline_session # Sagemakerのセッションを指定。パイプラインでのジョブの管理に使用。
)

logger.info("TrainingStepを定義しています。")
training_step = TrainingStep(
    name='TrainingStep',
    estimator=estimator,
    inputs={
        'train': TrainingInput(
            s3_data=processing_step.properties.ProcessingOutputConfig.Outputs['train-data'].S3Output.S3Uri,
            content_type='text/csv'
        )
    }
)








logger.info("パイプラインを定義しています。")
pipeline = Pipeline(
    name='LightGBMPipeline',
    parameters=[],
    steps=[processing_step,
           training_step,
           # evaluation_step,
           # register_step
          ],
    sagemaker_session=pipeline_session
)

### pipelineの実行
logger.info("パイプラインをアップサートしています。")
pipeline.upsert(role_arn=role)

logger.info("パイプラインを実行しています。")
execution = pipeline.start()






