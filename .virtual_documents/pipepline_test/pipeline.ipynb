


import pandas as pd
import numpy as np
import boto3
import sagemaker
from sagemaker.session import Session
from sagemaker.workflow.pipeline import Pipeline
from sagemaker.workflow.steps import ProcessingStep, TrainingStep
from sagemaker.processing import ScriptProcessor
from sagemaker.sklearn.processing import SKLearnProcessor
from sagemaker.workflow.pipeline_context import PipelineSession
from sagemaker.inputs import TrainingInput
from sagemaker.workflow.step_collections import RegisterModel
from sagemaker.sklearn.estimator import SKLearn
from sagemaker.model_metrics import MetricsSource, ModelMetrics
from sagemaker.workflow.parameters import ParameterString

from sagemaker.sklearn.processing import SKLearnProcessor
from sagemaker.workflow.steps import ProcessingStep
from sagemaker.sklearn.estimator import SKLearn
from sagemaker.inputs import TrainingInput
from sagemaker.processing import ScriptProcessor
from sagemaker.workflow.steps import TrainingStep
from sagemaker.sklearn.model import SKLearnModel
from sagemaker.workflow.functions import Join
from sagemaker.workflow.step_collections import RegisterModel

from sagemaker.workflow.pipeline import Pipeline


import os
import logging

# ログの設定
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)





from sklearn.datasets import make_classification

logger.info("サンプルデータを生成しています。")
# サンプルデータの生成
X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)
data = pd.DataFrame(X)
data['label'] = y

# データの保存
data_file = 'data.csv'
data.to_csv(data_file, index=False)
logger.info(f"サンプルデータを {data_file} に保存しました。")








# SageMakerセッションの作成
logger.info("SageMakerセッションを作成しています。")
session = sagemaker.Session()
region = session.boto_region_name
bucket = session.default_bucket()
role = sagemaker.get_execution_role()
s3_prefix = 'lightgbm-pipeline'

# パイプラインセッションの作成
pipeline_session = PipelineSession()





# データをS3にアップロード
logger.info("データをS3にアップロードしています。")
s3_uri = session.upload_data(path=data_file, bucket=bucket, key_prefix=f'{s3_prefix}/data')
logger.info(f"データが {s3_uri} にアップロードされました。")








### SKLearnProcessorを使用して前処理ステップを設定
logger.info("SKLearnProcessorを設定しています。")
sklearn_processor = SKLearnProcessor(
    framework_version='0.23-1',
    role=role,
    instance_type='ml.m5.xlarge',
    instance_count=1,
    base_job_name='lightgbm-preprocessing',
    sagemaker_session=pipeline_session
)

### ProessingStepを定義
logger.info("ProcessingStepを定義しています。")
processing_step = ProcessingStep(
    name='PreprocessingStep',
    processor=sklearn_processor,
    inputs=[
        sagemaker.processing.ProcessingInput(
            source=s3_uri, # 入力データのS3パスをs3_uriから取得する。
            destination='/opt/ml/processing/input/', # Processingジョブ内でのデータのディレクトリを指定。
            input_name='input-data' # この入力を"input_data"という名前で参照する。
        )
    ],
    
    outputs=[
        sagemaker.processing.ProcessingOutput(
            source='/opt/ml/processing/train',# Processingジョブ内での出力データの場所
            destination=f's3://{bucket}/{s3_prefix}/processing/train', # 出力データを保存するS3のパス
            output_name='train-data' # 出力データの名前
        ),
        sagemaker.processing.ProcessingOutput(
            source='/opt/ml/processing/test',
            destination=f's3://{bucket}/{s3_prefix}/processing/test',
            output_name='test-data'
        ),
    ],
    code='preprocessing.py',
)





logger.info("SKLearn Estimatorを設定しています。")
estimator = SKLearn(
    entry_point='train.py',
    role=role,
    instance_type='ml.m5.xlarge',
    instance_count=1,
    framework_version='0.23-1', # 使用するscikite-learnのバージョン指定
    py_version='py3',
    source_dir='.', # エントリーポイントスクリプトとその依存関係が含まれるディレクトリ
    dependencies=['requirements.txt'],
    output_path=f's3://{bucket}/{s3_prefix}/training_output', # トレーニングの結果を保存するS3のパス
    base_job_name='lightgbm-training', # トレーニングジョブのベース名。ジョブの識別に使用 
    sagemaker_session=pipeline_session # Sagemakerのセッションを指定。パイプラインでのジョブの管理に使用。
)

logger.info("TrainingStepを定義しています。")
training_step = TrainingStep(
    name='TrainingStep',
    estimator=estimator,
    inputs={
        'train': TrainingInput(
            s3_data=processing_step.properties.ProcessingOutputConfig.Outputs['train-data'].S3Output.S3Uri, #前処理ステップの出力であrトレーニングデータのS3パスを指定。
            content_type='text/csv'
        )
    }
)





# logger.info("評価用のScriptProcessorを設定しています。")
# evaluation_processor = ScriptProcessor(
#     image_uri=estimator.image_uri,
#     command=['python3'],
#     role=role,
#     instance_count=1,
#     instance_type='ml.m5.xlarge',
#     sagemaker_session=pipeline_session
# )

# logger.info("EvaluationStepを定義しています。")
# evaluation_step = ProcessingStep(
#     name='EvaluationStep',
#     processor=evaluation_processor,
#     inputs=[
#         sagemaker.processing.ProcessingInput(
#             source=training_step.properties.ModelArtifacts.S3ModelArtifacts,
#             destination='/opt/ml/processing/model'
#         ),
#         sagemaker.processing.ProcessingInput(
#             source=processing_step.properties.ProcessingOutputConfig.Outputs['test-data'].S3Output.S3Uri,
#             destination='/opt/ml/processing/test'
#         )
#     ],
#     outputs=[
#         sagemaker.processing.ProcessingOutput(
#             output_name='evaluation',
#             source='/opt/ml/processing/evaluation',
#             destination=f's3://{bucket}/{s3_prefix}/evaluation'
#         )
#     ],
#     code='evaluate.py'
# )



logger.info("評価用のScriptProcessorを設定しています。")
evaluation_processor = ScriptProcessor(
    image_uri=estimator.image_uri,
    command=['python3'],
    role=role,
    instance_count=1,
    instance_type='ml.m5.xlarge',
    base_job_name='lightgbm-evaluation',
    sagemaker_session=pipeline_session
)

logger.info("EvaluationStepを定義しています。")
evaluation_step = ProcessingStep(
    name='EvaluationStep',
    processor=evaluation_processor,
    code='evaluate.py',
    inputs=[
        sagemaker.processing.ProcessingInput(
            source=training_step.properties.ModelArtifacts.S3ModelArtifacts,
            destination='/opt/ml/processing/model'
        ),
        sagemaker.processing.ProcessingInput(
            source=processing_step.properties.ProcessingOutputConfig.Outputs['test-data'].S3Output.S3Uri,
            destination='/opt/ml/processing/test'
        ),
        sagemaker.processing.ProcessingInput(
            source='requirements.txt',
            destination='/opt/ml/processing/input',  # ディレクトリを指定
            input_name='requirements'
        )
    ],
    outputs=[
        sagemaker.processing.ProcessingOutput(
            output_name='evaluation',
            source='/opt/ml/processing/evaluation',
            destination=f's3://{bucket}/{s3_prefix}/evaluation'
        )
    ],
    job_arguments=['--model-dir', '/opt/ml/processing/model',
                   '--test-dir', '/opt/ml/processing/test',
                   '--output-dir', '/opt/ml/processing/evaluation']
)





logger.info("モデルメトリクスを設定しています。")
model_metrics = ModelMetrics(
    model_statistics=MetricsSource(
        s3_uri=Join(on='/', values=[
            evaluation_step.properties.ProcessingOutputConfig.Outputs['evaluation'].S3Output.S3Uri,
            'evaluation.json'
        ]),
        content_type='application/json'
    )
)

logger.info("SKLearnModelを作成しています。")
model = SKLearnModel(
    model_data=training_step.properties.ModelArtifacts.S3ModelArtifacts,
    role=role,
    entry_point='inference.py',
    source_dir='.',
    framework_version='0.23-1',
    py_version='py3',
    sagemaker_session=pipeline_session,
    env={
        'SAGEMAKER_REQUIREMENTS': 'requirements.txt'  # ここでrequirements.txtを指定
    }
)

logger.info("RegisterModelステップを定義しています。")
register_step = RegisterModel(
    name='ModelRegistration',
    estimator=estimator,
    model_data=training_step.properties.ModelArtifacts.S3ModelArtifacts,
    content_types=['text/csv'],
    response_types=['text/csv'],
    inference_instances=['ml.t2.medium', 'ml.m5.xlarge'],
    transform_instances=['ml.m5.xlarge'],
    model_package_group_name='LightGBMPackageGroup',
    model_metrics=model_metrics,
    approval_status='Approved',
)





logger.info("パイプラインを定義しています。")
pipeline = Pipeline(
    name='LightGBMPipeline',
    parameters=[],
    steps=[processing_step,
           training_step,
           evaluation_step,
           register_step
          ],
    sagemaker_session=pipeline_session
)

### pipelineの実行
logger.info("パイプラインをアップサートしています。")
pipeline.upsert(role_arn=role)

logger.info("パイプラインを実行しています。")
execution = pipeline.start()














import boto3
import sagemaker
from sagemaker import ModelPackage
from sagemaker.predictor import Predictor
from sagemaker.serializers import CSVSerializer
from sagemaker.deserializers import JSONDeserializer
from sagemaker import image_uris

# # セッションとロールの設定
# session = sagemaker.Session()
# region = session.boto_region_name
# sagemaker_client = boto3.client('sagemaker', region_name=region)
# role = sagemaker.get_execution_role()

# モデルパッケージのARNを取得
model_package_group_name = 'LightGBMPackageGroup'
response = sagemaker_client.list_model_packages(
    ModelPackageGroupName=model_package_group_name,
    MaxResults=1,
    SortBy='CreationTime',
    SortOrder='Descending'
)
model_package_arn = response['ModelPackageSummaryList'][0]['ModelPackageArn']
print(f"モデルパッケージのARN: {model_package_arn}")

# モデルパッケージを使用してエンドポイントをデプロイ
container = image_uris.retrieve(framework='lightgbm', region=session.boto_region_name)

model = ModelPackage(
    role=role,
    model_package_arn=model_package_arn,
    sagemaker_session=session,
    image_uri=container,
)

predictor = model.deploy(
    initial_instance_count=1,
    instance_type='ml.m5.xlarge',
    endpoint_name='lightgbm-endpoint-ver4'
)


status





# 推論に使用するデータの準備
import pandas as pd

# テストデータの取得
test_s3_uri = f's3://{bucket}/{s3_prefix}/processing/test/test.csv'
test_data = pd.read_csv(test_s3_uri)
X_test = test_data.drop('label', axis=1)

# テストデータの一部を取得
sample_data = X_test.head(5)

# 予測の実行
predictor.serializer = CSVSerializer()
predictor.deserializer = JSONDeserializer()

response = predictor.predict(sample_data.to_csv(header=False, index=False))
print(f"予測結果: {response}")



