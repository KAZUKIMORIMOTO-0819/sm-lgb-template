


# SageMaker関連のモジュール
import sagemaker
from sagemaker.session import Session  # SageMakerセッションの生成
from sagemaker.sklearn import SKLearnModel  # Scikit-learnモデル用のモジュール
from sagemaker.async_inference import AsyncInferenceConfig  # 非同期推論設定
from sagemaker.workflow.pipeline import Pipeline  # SageMaker Pipelineの設定
from sagemaker.workflow.steps import ProcessingStep, TrainingStep  # パイプラインのステップ
from sagemaker.processing import ScriptProcessor  # スクリプトプロセッサ
from sagemaker.sklearn.processing import SKLearnProcessor  # Scikit-learn用のプロセッサ
from sagemaker.workflow.pipeline_context import PipelineSession  # パイプラインセッション
from sagemaker.inputs import TrainingInput  # トレーニングデータの入力
from sagemaker.workflow.step_collections import RegisterModel  # モデル登録
from sagemaker.model_metrics import MetricsSource, ModelMetrics  # モデルメトリクス
from sagemaker.workflow.parameters import ParameterString  # パイプラインパラメータ
from sagemaker.workflow.functions import Join  # ステップ間でのパス結合
from sagemaker.model import Model
from sagemaker import get_execution_role
from sagemaker.image_uris import retrieve
from sagemaker.predictor import Predictor
from sagemaker.serializers import JSONSerializer
from sagemaker.deserializers import JSONDeserializer

# 必要な標準ライブラリ
import os  # OS操作
import json  # JSON操作
import logging  # ロギング設定
from io import BytesIO  # バイトストリーム
from time import sleep  # 一時停止処理
from uuid import uuid4  # UUID生成
from typing import Final  # 定数宣言用
import tarfile

# AWS関連モジュール
import boto3  # AWS SDK for Python

# データ分析・機械学習関連
import numpy as np  # 数値計算ライブラリ
import pandas as pd  # データ処理・分析
import lightgbm as lgb  # LightGBMモデル
from sklearn.model_selection import train_test_split  # データ分割
from sklearn.datasets import make_classification  # サンプルデータ生成

# ログの設定
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# コメント:
# - 不要な重複を排除し、モジュールを適切にまとめました。
# - import文をカテゴリーごとに整理することで、可読性を向上させています。
# - ロギング設定を追加し、loggerインスタンスを準備済み。








def initialize_sagemaker_settings():
    """
    SageMakerとAWSの事前設定を行う関数

    :return: 設定情報を含む辞書
    """
    # SageMakerクライアントとセッションの初期化
    smr_client: Final = boto3.client('sagemaker-runtime')
    sm_client: Final = boto3.client('sagemaker')
    s3_client: Final = boto3.client('s3')
    session = sagemaker.Session()

    # エンドポイントの状態を確認するためのウェイター設定
    endpoint_inservice_waiter: Final = sm_client.get_waiter('endpoint_in_service')

    # SageMakerの実行ロールとリージョンの取得
    role: Final[str] = sagemaker.get_execution_role()
    region: Final[str] = session.boto_region_name

    # デフォルトのS3バケットとプレフィックスの設定
    bucket: Final[str] = session.default_bucket()
    s3_prefix = 'lightgbm-model-deploy'

    # 設定情報のロギング
    logger.info(f"Initialized SageMaker settings with role={role}, region={region}, bucket={bucket}, s3_prefix={s3_prefix}")

    return {
        "smr_client": smr_client,
        "sm_client": sm_client,
        "s3_client": s3_client,
        "endpoint_inservice_waiter": endpoint_inservice_waiter,
        "role": role,
        "region": region,
        "bucket": bucket,
        "session": session,
        "s3_prefix": s3_prefix
    }

# 設定の初期化
sagemaker_settings = initialize_sagemaker_settings()





def create_directory(directory_path: str):
    """
    指定されたディレクトリを作成する関数。
    既に存在する場合は削除して新しく作成します。

    :param directory_path: 作成するディレクトリのパス
    """
    # 既存ディレクトリの削除
    if os.path.exists(directory_path):
        logger.info(f"Directory '{directory_path}' already exists. Removing it.")
        os.rmdir(directory_path)

    # ディレクトリの作成
    os.makedirs(directory_path, exist_ok=True)
    logger.info(f"Directory '{directory_path}' created successfully.")

# 定数としてディレクトリ名を定義
model_dir: Final[str] = 'model'
source_dir: Final[str] = 'source'

# 必要なディレクトリを作成
create_directory(model_dir)
create_directory(source_dir)





def generate_sample_data(n_samples: int = 1000, n_features: int = 20, n_classes: int = 2, random_state: int = 42, output_file: str = 'data.csv'):
    """
    サンプルデータを生成し、CSVファイルに保存する関数。

    :param n_samples: 生成するサンプル数 (デフォルト: 1000)
    :param n_features: 特徴量の数 (デフォルト: 20)
    :param n_classes: クラス数 (デフォルト: 2)
    :param random_state: 乱数シード (デフォルト: 42)
    :param output_file: 保存するCSVファイル名 (デフォルト: 'data.csv')
    """
    logger.info("Generating sample data...")

    # サンプルデータの生成
    X, y = make_classification(n_samples=n_samples, n_features=n_features, n_classes=n_classes, random_state=random_state)
    data = pd.DataFrame(X, columns=[f"feature_{i}" for i in range(n_features)])  # 特徴量に列名を付ける
    data['label'] = y

    # データの保存
    data.to_csv(output_file, index=False)
    logger.info(f"Sample data saved to {output_file}")

    return data

# サンプルデータの生成と保存
data = generate_sample_data()





def upload_to_s3(file_path: str, bucket_name: str, s3_prefix: str, session: Session) -> str:
    """
    ローカルファイルをS3にアップロードする関数。

    :param file_path: アップロードするローカルファイルのパス
    :param bucket_name: アップロード先のS3バケット名
    :param s3_prefix: S3上の保存先プレフィックス
    :param session: SageMakerのセッション
    :return: アップロードされたファイルのS3 URI
    """
    logger.info(f"Uploading {file_path} to S3 bucket '{bucket_name}' with prefix '{s3_prefix}'.")
    
    # S3にデータをアップロード
    s3_uri = session.upload_data(path=file_path,
                                 bucket=bucket_name,
                                 key_prefix=f'{s3_prefix}/data')
    
    logger.info(f"Data has been uploaded to {s3_uri}")
    return s3_uri

# データをS3にアップロード
data_file = "data.csv"
s3_uri = upload_to_s3(file_path=data_file,
                      bucket_name=sagemaker_settings["bucket"],
                      s3_prefix=sagemaker_settings["s3_prefix"],
                      session=sagemaker_settings["session"])








def split_train_test(data: pd.DataFrame, test_size: float = 0.2, random_state: int = 42) -> tuple:
    """
    データを訓練データとテストデータに分割する関数。

    :param data: 分割するデータフレーム
    :param test_size: テストデータの割合 (デフォルト: 0.2)
    :param random_state: ランダムシード (デフォルト: 42)
    :return: 訓練データとテストデータのタプル (train, test)
    """
    logger.info(f"Splitting data into train and test sets with test_size={test_size} and random_state={random_state}")
    
    # データの分割
    train, test = train_test_split(data, test_size=test_size, random_state=random_state)
    
    logger.info(f"Data split completed: {len(train)} training samples, {len(test)} test samples")
    
    return train, test

# 使用例
data = pd.read_csv('data.csv')  # データを読み込む場合の例
train, test = split_train_test(data)





def train_lightgbm_model(train_data: pd.DataFrame, label_column: str = 'label', params: dict = None) -> lgb.Booster:
    """
    LightGBMモデルを訓練する関数

    :param train_data: 訓練データを含むDataFrame
    :param label_column: 目的変数の列名 (デフォルト: 'label')
    :param params: LightGBMのハイパーパラメータ (デフォルトはバイナリ分類用)
    :return: 訓練済みLightGBMモデル (lgb.Boosterオブジェクト)
    """
    logger.info('Preparing training data for LightGBM model.')

    # 訓練データとラベルの分割
    X_train = train_data.drop(label_column, axis=1)
    y_train = train_data[label_column]

    logger.info('Training LightGBM model...')
    
    # LightGBM用のデータセットを作成
    train_dataset = lgb.Dataset(X_train, label=y_train)

    # デフォルトのハイパーパラメータを設定（必要に応じて変更可能）
    if params is None:
        params = {
            'objective': 'binary',  # バイナリ分類用
            'metric': 'binary_logloss',  # 評価指標
            'verbosity': -1  # 詳細出力の抑制
        }
    
    # モデルの訓練
    model = lgb.train(params, train_dataset)
    
    logger.info('LightGBM model training completed.')
    
    return model

# 使用例
# 訓練データを分割済みの場合
trained_model = train_lightgbm_model(train)





def save_and_compress_model(model: lgb.Booster, model_dir: str = 'model', model_filename: str = 'model.txt', archive_filename: str = 'model.tar.gz'):
    """
    モデルを保存し、ディレクトリ内のファイルを圧縮する関数

    :param model: LightGBM Boosterオブジェクト
    :param model_dir: モデルを保存するディレクトリ (デフォルト: 'model')
    :param model_filename: 保存するモデルファイル名 (デフォルト: 'model.txt')
    :param archive_filename: 圧縮するアーカイブファイル名 (デフォルト: 'model.tar.gz')
    """
    # ディレクトリが存在しない場合は作成
    if not os.path.exists(model_dir):
        os.makedirs(model_dir)
        logger.info(f"Directory '{model_dir}' created.")

    # モデルの保存
    model_path = os.path.join(model_dir, model_filename)
    model.save_model(model_path)
    logger.info(f"Model saved to {model_path}")

    # モデルディレクトリを圧縮
    archive_path = os.path.join(model_dir, archive_filename)
    with tarfile.open(archive_path, 'w:gz') as tar:
        tar.add(model_dir, arcname='.')
    logger.info(f"Model directory '{model_dir}' compressed to '{archive_path}'")

# 使用例
# 事前に訓練したLightGBMモデルを保存・圧縮する場合
save_and_compress_model(trained_model)





def upload_model_to_s3(model_path: str, s3_prefix: str, session: Session, bucket_name: str) -> str:
    """
    圧縮したモデルファイルをS3にアップロードする関数

    :param model_path: アップロードするモデルファイルのローカルパス
    :param s3_prefix: S3上の保存先プレフィックス
    :param session: SageMakerのセッション
    :param bucket_name: アップロード先のS3バケット名
    :return: アップロードされたモデルファイルのS3 URI
    """
    logger.info(f"Uploading model to S3: {model_path} -> s3://{bucket_name}/{s3_prefix}")
    
    # モデルファイルをS3にアップロード
    model_s3_uri: Final[str] = session.upload_data(
        path=model_path,
        bucket=bucket_name,
        key_prefix=s3_prefix
    )
    
    logger.info(f"Model uploaded to S3 at {model_s3_uri}")
    return model_s3_uri

# 使用例
model_dir = 'model'  # モデルディレクトリ
model_tar_path = f'./{model_dir}/model.tar.gz'

# モデルをS3にアップロード
model_s3_uri = upload_model_to_s3(model_path=model_tar_path, 
                                  s3_prefix=sagemaker_settings["s3_prefix"], 
                                  session=sagemaker_settings["session"], 
                                  bucket_name=sagemaker_settings["bucket"])
print(model_s3_uri)





def deploy_model_to_sagemaker(model_s3_uri: str, role: str, entry_point: str, source_dir: str, session: Session,
                              framework: str = "pytorch", version: str = "2.1", py_version: str = "py310",
                              instance_type: str = "ml.m5.large", initial_instance_count: int = 1,
                              endpoint_name: str = "lightgbm-endpoint") -> Model:
    """
    SageMakerにモデルをデプロイする関数

    :param model_s3_uri: S3上のモデルアーティファクトのURI
    :param role: SageMaker用のIAMロール
    :param entry_point: 推論用スクリプトのパス
    :param source_dir: entry_pointを含むディレクトリ
    :param session: SageMakerのセッション
    :param framework: 使用するフレームワーク (デフォルト: 'pytorch')
    :param version: フレームワークのバージョン (デフォルト: '2.1')
    :param py_version: Pythonバージョン (デフォルト: 'py310')
    :param instance_type: デプロイ用のインスタンスタイプ (デフォルト: 'ml.m5.large')
    :param initial_instance_count: デプロイ用のインスタンス数 (デフォルト: 1)
    :param endpoint_name: デプロイするエンドポイントの名前 (デフォルト: 'lightgbm-endpoint')
    :return: デプロイされたモデルのPredictorオブジェクト
    """
    # コンテナURIの取得
    logger.info(f"Retrieving container URI for framework={framework}, version={version}, region={session.boto_region_name}")
    container_uri = retrieve(
        framework=framework,
        region=session.boto_region_name,
        version=version,
        py_version=py_version,
        image_scope="inference",
        instance_type=instance_type,
    )

    # Modelオブジェクトの作成
    logger.info("Creating SageMaker model object...")
    model = Model(
        model_data=model_s3_uri,
        role=role,
        entry_point=entry_point,
        source_dir=source_dir,
        image_uri=container_uri,
        sagemaker_session=session
    )

    # エンドポイントのデプロイ
    logger.info(f"Deploying model to endpoint {endpoint_name} with instance type {instance_type}...")
    predictor = model.deploy(
        initial_instance_count=initial_instance_count,
        instance_type=instance_type,
        endpoint_name=endpoint_name
    )

    logger.info(f"Model deployed successfully at endpoint: {endpoint_name}")
    return predictor

# 使用例
entry_point = 'inference.py'
source_dir = 'source'  # 推論スクリプトと依存ファイルを含むディレクトリ
endpoint_name = "lightgbm-deploy-template"
predictor = deploy_model_to_sagemaker(model_s3_uri=model_s3_uri,
                                      role = sagemaker_settings["role"],
                                      entry_point = entry_point,
                                      source_dir = source_dir,
                                      session = sagemaker_settings["session"],
                                      endpoint_name = endpoint_name)





def test_endpoint_prediction(endpoint_name: str, input_data: dict, session, serializer=JSONSerializer(), deserializer=JSONDeserializer()) -> dict:
    """
    デプロイされたエンドポイントに対して推論を実行する関数

    :param endpoint_name: デプロイされたエンドポイントの名前
    :param input_data: 推論に使用するデータ
    :param session: SageMakerセッション
    :param serializer: 入力データのシリアライザ (デフォルト: JSONSerializer)
    :param deserializer: 出力データのデシリアライザ (デフォルト: JSONDeserializer)
    :return: エンドポイントからの推論結果
    """
    logger.info(f"Creating Predictor object for endpoint '{endpoint_name}'")

    # Predictorオブジェクトの作成
    predictor = Predictor(
        endpoint_name=endpoint_name,
        sagemaker_session=session,
        serializer=serializer,
        deserializer=deserializer
    )

    logger.info("Sending data to the endpoint for prediction")
    
    # 推論の実行
    response = predictor.predict(input_data)
    
    logger.info("Prediction completed")
    return response



# 使用例
input_data = {
    "data": [
        [0.09337237, 0.78584826, 0.10575379, 1.2723535, -0.84631598,
         -0.97909326, 1.26370668, 0.26402008, 2.41167668, -0.9600463,
         0.54347938, 0.19981043, 0.28872366, 0.7324921, -0.87200205,
         -1.65488744, -1.13020372, -0.12270893, 0.6934308, 0.91136272]
    ]
}

# エンドポイントでの推論実行
session = sagemaker_settings["session"]  # 事前に初期化したSageMaker設定を使用
prediction_result = test_endpoint_prediction(endpoint_name, input_data, session)

# 結果の表示
print(prediction_result['predictions'])



