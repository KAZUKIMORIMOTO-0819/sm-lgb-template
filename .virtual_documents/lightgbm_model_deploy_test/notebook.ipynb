


import sagemaker
from typing import Final
from sagemaker.sklearn import SKLearnModel
from sagemaker.async_inference import AsyncInferenceConfig
import os, boto3, json, numpy as np
from io import BytesIO
from time import sleep
from uuid import uuid4

import pandas as pd
import numpy as np
import boto3
import sagemaker
from sagemaker.session import Session
from sagemaker.workflow.pipeline import Pipeline
from sagemaker.workflow.steps import ProcessingStep, TrainingStep
from sagemaker.processing import ScriptProcessor
from sagemaker.sklearn.processing import SKLearnProcessor
from sagemaker.workflow.pipeline_context import PipelineSession
from sagemaker.inputs import TrainingInput
from sagemaker.workflow.step_collections import RegisterModel
from sagemaker.sklearn.estimator import SKLearn
from sagemaker.model_metrics import MetricsSource, ModelMetrics
from sagemaker.workflow.parameters import ParameterString

from sagemaker.sklearn.processing import SKLearnProcessor
from sagemaker.workflow.steps import ProcessingStep
from sagemaker.sklearn.estimator import SKLearn
from sagemaker.inputs import TrainingInput
from sagemaker.processing import ScriptProcessor
from sagemaker.workflow.steps import TrainingStep
from sagemaker.sklearn.model import SKLearnModel
from sagemaker.workflow.functions import Join
from sagemaker.workflow.step_collections import RegisterModel

from sagemaker.workflow.pipeline import Pipeline

from sklearn.datasets import make_classification

import os
import logging
import lightgbm as lgb
from sklearn.model_selection import train_test_split
# ログの設定
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)








smr_client:Final = boto3.client('sagemaker-runtime')
sm_client:Final = boto3.client('sagemaker')
s3_client:Final = boto3.client('s3')
endpoint_inservice_waiter:Final = sm_client.get_waiter('endpoint_in_service')
role: Final[str] = sagemaker.get_execution_role()
region: Final[str] = sagemaker.Session().boto_region_name
bucket: Final[str] = sagemaker.Session().default_bucket()
session = sagemaker.Session()
s3_prefix = 'lightgbm-model-deploy'





model_dir: Final[str] = 'model'
get_ipython().getoutput("if [ -d ./{model_dir} ]; then rm -rf ./{model_dir}/;fi")
get_ipython().getoutput("mkdir ./{model_dir}/")

source_dir: Final[str] = 'source'
get_ipython().getoutput("if [ -d ./{source_dir} ]; then rm -rf ./{source_dir}/;fi")
get_ipython().getoutput("mkdir ./{source_dir}/")





logger.info("サンプルデータを生成しています。")
# サンプルデータの生成
X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)
data = pd.DataFrame(X)
data['label'] = y

# データの保存
data_file = 'data.csv'
data.to_csv(data_file, index=False)
logger.info(f"サンプルデータを {data_file} に保存しました。")





# データをS3にアップロード
logger.info("データをS3にアップロードしています。")
s3_uri = session.upload_data(path=data_file, 
                             bucket=bucket, 
                             key_prefix=f'{s3_prefix}/data')
logger.info(f"データが {s3_uri} にアップロードされました。")








train, test = train_test_split(data, test_size=0.2, random_state=42)





X_train = train.drop('label', axis=1)
y_train = train['label']

logger.info('LightGBMモデルを訓練しています。')
train_dataset = lgb.Dataset(X_train, 
                            label=y_train)
params = {
    'objective': 'binary',
    'metric': 'binary_logloss',
    'verbosity': -1
}
model = lgb.train(params, train_dataset)





model.save_model('model/model.txt')





get_ipython().run_line_magic("cd", " {model_dir}")
get_ipython().getoutput("tar zcvf model.tar.gz ./*")
get_ipython().run_line_magic("cd", " ..")





model_s3_uri:Final[str] = sagemaker.session.Session().upload_data(
    f'./{model_dir}/model.tar.gz',
    key_prefix = f'{s3_prefix}'
)
print(model_s3_uri)





from sagemaker.model import Model
from sagemaker import Session
from sagemaker import get_execution_role
from sagemaker.image_uris import retrieve

# SageMakerセッションとIAMロールの設定
session = Session()
role = get_execution_role()

# LightGBM用のコンテナURIを取得
# container = retrieve(framework="sagemaker-data-science-311-v1",  # 'lightgbm'が直接サポートされていない場合
#                     region=session.boto_region_name,
#                     version='3.9')
container = retrieve(
    framework="pytorch",
    region=session.boto_region_name,
    version="2.1",
    py_version="py310",
    image_scope="inference",
    instance_type="ml.m5.large",
)


# Modelオブジェクトの作成
model = Model(
    model_data=model_s3_uri,
    role=role,
    entry_point='inference.py',
    source_dir='source_dir',  # inference.pyとrequirements.txtが含まれるディレクトリ
    image_uri=container,
    sagemaker_session=session
)

# エンドポイントのデプロイ
predictor = model.deploy(
    initial_instance_count=1,
    instance_type='ml.m5.xlarge',
    endpoint_name='lightgbm-endpoint-ver6'
)





import json
import sagemaker
from sagemaker.predictor import Predictor
from sagemaker.serializers import JSONSerializer
from sagemaker.deserializers import JSONDeserializer

# SageMakerセッションの初期化
session = sagemaker.Session()

# エンドポイント名を指定
endpoint_name = 'lightgbm-endpoint-ver6'  # 実際のエンドポイント名に置き換えてください

# Predictorオブジェクトの作成
predictor = Predictor(
    endpoint_name=endpoint_name,
    sagemaker_session=session,
    serializer=JSONSerializer(),      # 入力データのシリアライズ方法
    deserializer=JSONDeserializer()   # 出力データのデシリアライズ方法
)

# 入力データの準備
input_data = {
    "data": [
        [ 0.09337237,  0.78584826,  0.10575379,  1.2723535 , -0.84631598,
       -0.97909326,  1.26370668,  0.26402008,  2.41167668, -0.9600463 ,
        0.54347938,  0.19981043,  0.28872366,  0.7324921 , -0.87200205,
       -1.65488744, -1.13020372,-0.12270893,0.6934308,0.91136272] # 実際の特徴量に置き換えてください
    ]
}

# 推論の実行
response = predictor.predict(input_data)

# 結果の表示
print(response['predictions'])




