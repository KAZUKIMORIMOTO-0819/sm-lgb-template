








import sagemaker
from typing import Final
from sagemaker.sklearn import SKLearnModel
from sagemaker.async_inference import AsyncInferenceConfig
import os, boto3, json, numpy as np
from io import BytesIO
from time import sleep
from uuid import uuid4
smr_client:Final = boto3.client('sagemaker-runtime')
sm_client:Final = boto3.client('sagemaker')
s3_client:Final = boto3.client('s3')
endpoint_inservice_waiter:Final = sm_client.get_waiter('endpoint_in_service')
role: Final[str] = sagemaker.get_execution_role()
region: Final[str] = sagemaker.Session().boto_region_name
bucket: Final[str] = sagemaker.Session().default_bucket()





model_dir: Final[str] = 'model'
get_ipython().getoutput("if [ -d ./{model_dir} ]; then rm -rf ./{model_dir}/;fi")
get_ipython().getoutput("mkdir ./{model_dir}/")

source_dir: Final[str] = 'source'
get_ipython().getoutput("if [ -d ./{source_dir} ]; then rm -rf ./{source_dir}/;fi")
get_ipython().getoutput("mkdir ./{source_dir}/")








get_ipython().run_cell_magic("writefile", " ./{model_dir}/my_model.txt", """Hello my great machine learning model""")





get_ipython().run_line_magic("cd", " {model_dir}")
get_ipython().getoutput("tar zcvf model.tar.gz ./*")
get_ipython().run_line_magic("cd", " ..")





model_s3_uri:Final[str] = sagemaker.session.Session().upload_data(
    f'./{model_dir}/model.tar.gz',
    key_prefix = 'hello_sagemaker_inference'
)
print(model_s3_uri)





get_ipython().run_cell_magic("writefile", " ./{source_dir}/inference.py", """import os
def model_fn(model_dir):
    with open(os.path.join(model_dir,'my_model.txt')) as f:
        model = f.read()[:-1]
    return model
def predict_fn(input_data, model):
    response = f'{model} for the {input_data}st time'
    return response""")





# 名前の設定
model_name: Final[str] = 'SKLearnModel'
endpoint_name: Final[str] = model_name + 'Endpoint'


# モデルとコンテナの指定
sklearn_model = SKLearnModel(
    name = model_name,
    model_data=model_s3_uri,
    role= role,
    framework_version = '1.0-1',
    py_version='py3',
    entry_point='inference.py',
    source_dir=f'./{source_dir}/'
)
sklearn_predictor = sklearn_model.deploy(
    initial_instance_count=1,
    instance_type='ml.m5.large',
    enpoint_name=endpoint_name
)


response = sklearn_predictor.predict(1)
print(response,type(response))





sklearn_predictor.serializer, sklearn_predictor.deserializer


# endpointとモデルを削除
sklearn_predictor.delete_endpoint()
sklearn_model.delete_model()








get_ipython().run_line_magic("cd", " {source_dir}")
get_ipython().getoutput("tar zcvf sourcedir.tar.gz ./*")
get_ipython().run_line_magic("cd", " ..")


source_s3_uri:Final[str] = sagemaker.session.Session().upload_data(
    f'./{source_dir}/sourcedir.tar.gz',
    key_prefix = 'hello_sagemaker_inference'
)
print(source_s3_uri)





endpoint_config_name: Final[str] = model_name + 'EndpointConfig'


endpoint_config_name





# コンテナイメージの URI を取得
container_image_uri: Final[str] = sagemaker.image_uris.retrieve(
    "sklearn",  # SKLearn のマネージドコンテナを利用
    sagemaker.session.Session().boto_region_name, # ECR のリージョンを指定
    version='1.0-1', # SKLearn のバージョンを指定
    instance_type = 'ml.m5.large', # インスタンスタイプを指定
    image_scope = 'inference' # 推論コンテナを指定
)
print(container_image_uri)





# Model 作成
response = sm_client.create_model(
    ModelName=model_name,
    PrimaryContainer={
        'Image': container_image_uri,
        'ModelDataUrl': model_s3_uri,
        'Environment': {
            'SAGEMAKER_CONTAINER_LOG_LEVEL': '20',
            'SAGEMAKER_PROGRAM': 'inference.py',
            'SAGEMAKER_REGION': region,
            'SAGEMAKER_SUBMIT_DIRECTORY': source_s3_uri}
    },
    ExecutionRoleArn=role,
)
# EndpointConfig 作成
response = sm_client.create_endpoint_config(
    EndpointConfigName=endpoint_config_name,
    ProductionVariants=[
        {
            'VariantName': 'AllTrafic',
            'ModelName': model_name,
            'InitialInstanceCount': 1,
            'InstanceType': 'ml.m5.large',
        },
    ],
)
# Endpoint 作成
response = sm_client.create_endpoint(
    EndpointName=endpoint_name,
    EndpointConfigName=endpoint_config_name,
)
# Endpoint が有効化されるまで待つ
endpoint_inservice_waiter.wait(
    EndpointName=endpoint_name,
    WaiterConfig={'Delay': 5,}
)





response = smr_client.invoke_endpoint(
    EndpointName=endpoint_name,
    ContentType='application/json',
    Accept='application/json',
    Body='1'
)
predictions = json.loads(response['Body'].read().decode('utf-8'))
print(predictions)





# default_input_fn 確認
get_ipython().getoutput("curl -s https://raw.githubusercontent.com/aws/sagemaker-scikit-learn-container/master/src/sagemaker_sklearn_container/handler_service.py | pygmentize")


# default_input_fn が呼ぶ decode の確認
get_ipython().getoutput("curl -s https://raw.githubusercontent.com/aws/sagemaker-inference-toolkit/master/src/sagemaker_inference/decoder.py | pygmentize")


# content_types の確認
get_ipython().getoutput("curl -s https://raw.githubusercontent.com/aws/sagemaker-inference-toolkit/master/src/sagemaker_inference/content_types.py | pygmentize")





buffer = BytesIO()
np.save(buffer,np.array(1))
response = smr_client.invoke_endpoint(
    EndpointName=endpoint_name,
    ContentType='application/x-npy',
    Accept='application/json',
    Body=buffer.getvalue(),
)
predictions = json.loads(response['Body'].read().decode('utf-8'))
print(predictions)





sm_client.delete_endpoint(EndpointName=endpoint_name)
sm_client.delete_endpoint_config(EndpointConfigName=endpoint_config_name)
sm_client.delete_model(ModelName=model_name)


# 削除が完了するまで待つ
sleep(5)





get_ipython().run_cell_magic("writefile", " ./{source_dir}/inference.py", """import os, json
def model_fn(model_dir):
    with open(os.path.join(model_dir,'my_model.txt')) as f:
        hello = f.read()[:-1] # 改行を除外
    return hello
def input_fn(input_data, content_type):
    if content_type == 'text/csv':
        transformed_data = input_data.split(',')
    else:
        raise ValueError(f"Illegal content type {content_type}. The only allowed content_type is text/csv")
    print(input_data,transformed_data)
    return transformed_data
def predict_fn(transformed_data, model):
    prediction_list = []
    for data in transformed_data:
        if data[-1] == '1':
            ordinal = f'{data}st'
        elif data[-1] == '2':
            ordinal = f'{data}nd'
        elif data[-1] == '3':
            ordinal = f'{data}rd'
        else:
            ordinal = f'{data}th'
        prediction = f'{model} for the {ordinal} time'
        prediction_list.append(prediction)
    print(transformed_data,prediction_list)    
    return prediction_list
def output_fn(prediction_list, accept):
    if accept == 'text/csv':    
        response = ''
        for prediction in prediction_list:
            response += prediction + '\n'
        print(prediction_list,response)
    else:
        raise ValueError(f"Illegal accept type {accept}. The only allowed accept type is text/csv")
    return response, accept""")





get_ipython().run_line_magic("cd", " {source_dir}")
get_ipython().getoutput("tar zcvf sourcedir.tar.gz ./*")
get_ipython().run_line_magic("cd", " ..")


source_s3_uri:Final[str] = sagemaker.session.Session().upload_data(
    f'./{source_dir}/sourcedir.tar.gz',
    key_prefix = 'hello_sagemaker_inference'
)
print(source_s3_uri)





# Model 作成
response = sm_client.create_model(
    ModelName=model_name,
    PrimaryContainer={
        'Image': container_image_uri,
        'ModelDataUrl': model_s3_uri,
        'Environment': {
            'SAGEMAKER_CONTAINER_LOG_LEVEL': '20',
            'SAGEMAKER_PROGRAM': 'inference.py',
            'SAGEMAKER_REGION': region,
            'SAGEMAKER_SUBMIT_DIRECTORY': source_s3_uri}
    },
    ExecutionRoleArn=role,
)
# EndpointConfig 作成
response = sm_client.create_endpoint_config(
    EndpointConfigName=endpoint_config_name,
    ProductionVariants=[
        {
            'VariantName': 'AllTrafic',
            'ModelName': model_name,
            'InitialInstanceCount': 1,
            'InstanceType': 'ml.m5.large',
        },
    ],
)
# Endpoint 作成
response = sm_client.create_endpoint(
    EndpointName=endpoint_name,
    EndpointConfigName=endpoint_config_name,
)
# Endpoint が有効化されるまで待つ
endpoint_inservice_waiter.wait(
    EndpointName=endpoint_name,
    WaiterConfig={'Delay': 5,}
)





response = smr_client.invoke_endpoint(
    EndpointName=endpoint_name,
    ContentType='text/csv',
    Accept='text/csv',
    Body='1,2,3,10000'
)
predictions = response['Body'].read().decode('utf-8')
print(predictions)





sm_client.delete_endpoint(EndpointName=endpoint_name)
sm_client.delete_endpoint_config(EndpointConfigName=endpoint_config_name)
sm_client.delete_model(ModelName=model_name)


sleep(5)





# Model 作成
response = sm_client.create_model(
    ModelName=model_name,
    PrimaryContainer={
        'Image': container_image_uri,
        'ModelDataUrl': model_s3_uri,
        'Environment': {
            'SAGEMAKER_CONTAINER_LOG_LEVEL': '20',
            'SAGEMAKER_PROGRAM': 'inference.py',
            'SAGEMAKER_REGION': region,
            'SAGEMAKER_SUBMIT_DIRECTORY': source_s3_uri}
    },
    ExecutionRoleArn=role,
)
# EndpointConfig 作成
response = sm_client.create_endpoint_config(
    EndpointConfigName=endpoint_config_name,
    ProductionVariants=[
        {
            'VariantName': 'AllTrafic',
            'ModelName': model_name,
            'InitialInstanceCount': 1,
            'InstanceType': 'ml.m5.large',
        },
    ],
    AsyncInferenceConfig={
        "OutputConfig": {
            "S3OutputPath": f"s3://{bucket}/hello_sagemaker_inference/async_inference/output"
        },
    }
)
# Endpoint 作成
response = sm_client.create_endpoint(
    EndpointName=endpoint_name,
    EndpointConfigName=endpoint_config_name,
)
# Endpoint が有効化されるまで待つ
endpoint_inservice_waiter.wait(
    EndpointName=endpoint_name,
    WaiterConfig={'Delay': 5,}
)





input_data: Final[str] = './input_data.csv'
with open(input_data,'wt') as f:
    f.write('2,3,4,1000')





input_data_s3_uri:Final[str] = sagemaker.Session().upload_data(
    './input_data.csv',
    key_prefix = 'hello_sagemaker_inference/async_inference'
)
print(input_data_s3_uri)





response = smr_client.invoke_endpoint_async(
    EndpointName=endpoint_name, 
    InputLocation=input_data_s3_uri,
    ContentType='text/csv',
    Accept='text/csv',
)
output_s3_uri = response['OutputLocation']
output_key = output_s3_uri.replace(f's3://{bucket}/','')
while True:
    result = s3_client.list_objects(Bucket=bucket, Prefix=output_key)
    exists = True if "Contents" in result else False
    if exists:
        print('!')
        obj = s3_client.get_object(Bucket=bucket, Key=output_key)
        predictions = obj['Body'].read().decode()
        print(predictions)
        break
    else:
        print('.',end='')
        sleep(0.1)





sm_client.delete_endpoint(EndpointName=endpoint_name)
sm_client.delete_endpoint_config(EndpointConfigName=endpoint_config_name)
sm_client.delete_model(ModelName=model_name)


sleep(5)





# Model 作成
response = sm_client.create_model(
    ModelName=model_name,
    PrimaryContainer={
        'Image': container_image_uri,
        'ModelDataUrl': model_s3_uri,
        'Environment': {
            'SAGEMAKER_CONTAINER_LOG_LEVEL': '20',
            'SAGEMAKER_PROGRAM': 'inference.py',
            'SAGEMAKER_REGION': region,
            'SAGEMAKER_SUBMIT_DIRECTORY': source_s3_uri}
    },
    ExecutionRoleArn=role,
)
# EndpointConfig 作成
response = sm_client.create_endpoint_config(
    EndpointConfigName=endpoint_config_name,
    ProductionVariants=[
        {
            'ModelName': model_name,
            'VariantName': 'AllTrafic',
            'ServerlessConfig': { 
                'MemorySizeInMB': 1024, 
                'MaxConcurrency': 3
            }
        },
    ],
)
# Endpoint 作成
response = sm_client.create_endpoint(
    EndpointName=endpoint_name,
    EndpointConfigName=endpoint_config_name,
)
# Endpoint が有効化されるまで待つ
endpoint_inservice_waiter.wait(
    EndpointName=endpoint_name,
    WaiterConfig={'Delay': 5,}
)


response = smr_client.invoke_endpoint(
    EndpointName=endpoint_name,
    ContentType='text/csv',
    Accept='text/csv',
    Body='1,2,3,10000'
)
predictions = response['Body'].read().decode('utf-8')
print(predictions)


sm_client.delete_endpoint(EndpointName=endpoint_name)
sm_client.delete_endpoint_config(EndpointConfigName=endpoint_config_name)
sm_client.delete_model(ModelName=model_name)








get_ipython().getoutput("mkdir -p batch")
batch_data_dir: Final[str] = './batch'
input_data1: Final[str] = 'input_data1.csv'
input_data2: Final[str] = './input_data2.csv'
with open(os.path.join(batch_data_dir,input_data1),'wt') as f:
    f.write('3,4,5,100')
with open(os.path.join(batch_data_dir,input_data2),'wt') as f:
    f.write('9,8,7,6,5')
get_ipython().getoutput("aws s3 rm --recursive s3://{bucket}/{prefix}")
prefix:Final[str] = 'hello_sagemaker_inference/transform_job'
input_prefix:Final[str] = prefix + '/input'
output_prefix:Final[str] = prefix + '/output'
input_data_s3_uri:Final[str] = sagemaker.Session().upload_data(batch_data_dir,key_prefix = input_prefix)
print(input_data_s3_uri)





# Model 作成
response = sm_client.create_model(
    ModelName=model_name,
    PrimaryContainer={
        'Image': container_image_uri,
        'ModelDataUrl': model_s3_uri,
        'Environment': {
            'SAGEMAKER_CONTAINER_LOG_LEVEL': '20',
            'SAGEMAKER_PROGRAM': 'inference.py',
            'SAGEMAKER_REGION': region,
            'SAGEMAKER_SUBMIT_DIRECTORY': source_s3_uri}
    },
    ExecutionRoleArn=role,
)





transform_job_name: Final[str] = f'{model_name}TransformJob-{uuid4()}'
print(transform_job_name)
response = sm_client.create_transform_job(
    TransformJobName=transform_job_name,
    ModelName=model_name,
    TransformInput={
        'DataSource': {
            'S3DataSource': {
                'S3DataType': 'S3Prefix',
                'S3Uri': f's3://{bucket}/{input_prefix}'
            }
        },
        'ContentType': 'text/csv',
    },
    TransformOutput={
        'S3OutputPath': f's3://{bucket}/{output_prefix}',
        'Accept': 'text/csv',
    },
    TransformResources={
        'InstanceType': 'ml.m5.large',
        'InstanceCount': 1,
    }
)





while True:
    if sm_client.describe_transform_job(TransformJobName=transform_job_name)['TransformJobStatus'] == 'Completed':
        print('!')
        for content in s3_client.list_objects_v2(Bucket=bucket,Prefix=output_prefix)['Contents']:
            obj = s3_client.get_object(Bucket=bucket, Key=content['Key'])
            predictions = obj['Body'].read().decode()
            print(predictions)
        break
    else:
        print('.',end='')
        sleep(5)





# for endpoint in sm_client.list_endpoints()['Endpoints']:
#     response = sm_client.delete_endpoint(EndpointName=endpoint['EndpointName'])
#     print(response)


# for endpoint_config in sm_client.list_endpoint_configs()['EndpointConfigs']:
#     response = sm_client.delete_endpoint_config(EndpointConfigName=endpoint_config['EndpointConfigName'])
#     print(response)


# for model in sm_client.list_models()['Models']:
#     response = sm_client.delete_model(ModelName=model['ModelName'])
#     print(response)



