


import boto3
import sagemaker
from sagemaker import get_execution_role
from sagemaker.session import Session
from sagemaker.workflow.parameters import (
    ParameterString,
    ParameterInteger,
)
from sagemaker.workflow.steps import (
    ProcessingStep,
    TrainingStep,
)
from sagemaker.workflow.pipeline import Pipeline
from sagemaker.processing import ScriptProcessor, ProcessingInput, ProcessingOutput
from sagemaker.sklearn.estimator import SKLearn
from sagemaker.sklearn.model import SKLearnModel
from sagemaker.workflow.condition_step import ConditionStep
from sagemaker.workflow.conditions import ConditionGreaterThanOrEqualTo
from sagemaker.workflow.functions import JsonGet
from sagemaker.workflow.model_step import ModelStep
from sagemaker.model import Model
from sagemaker.serverless import ServerlessInferenceConfig
from sagemaker.workflow.properties import PropertyFile
import os
import pandas as pd
import numpy as np








# セッションとロールの設定
sagemaker_session = sagemaker.session.Session()
role = get_execution_role()
region = sagemaker_session.boto_region_name
default_bucket = sagemaker_session.default_bucket()

print(sagemaker_session)
print(role)
print(region)
print(default_bucket)





# パラメータの定義
processing_instance_type = ParameterString(name="ProcessingInstanceType", default_value="ml.m5.xlarge")
training_instance_type = ParameterString(name="TrainingInstanceType", default_value="ml.m5.xlarge")
model_approval_status = ParameterString(name="ModelApprovalStatus", default_value="Approved")
input_data_uri = f's3://{default_bucket}/lightgbm-pipeline/input/data.csv'





from sklearn.datasets import make_classification

X, y = make_classification(
    n_samples=1000, n_features=20, n_informative=15, n_redundant=5, n_classes=2, random_state=42
)
df = pd.DataFrame(X)
df['target'] = y

# データをS3にアップロード
os.makedirs('data', exist_ok=True)
df.to_csv('data/data.csv', index=False)
sagemaker_session.upload_data(path='data/data.csv', 
                              bucket=default_bucket, 
                              key_prefix='lightgbm-pipeline/input')











# 前処理スクリプトの作成
processing_script = """
import argparse
import os
import pandas as pd
import boto3
import logging
from sklearn.model_selection import train_test_split
from io import StringIO, BytesIO

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

if __name__ == "__main__":
    logger.info("前処理を開始します。")
    parser = argparse.ArgumentParser()
    parser.add_argument("--s3-bucket", type=str)
    parser.add_argument("--s3-key", type=str)
    parser.add_argument("--output-train", type=str, default="/opt/ml/processing/train")
    parser.add_argument("--output-validation", type=str, default="/opt/ml/processing/validation")
    args = parser.parse_args()

    logger.info("S3からデータを読み込んでいます。")
    s3 = boto3.client('s3')
    response = s3.get_object(Bucket=args.s3_bucket, Key=args.s3_key)
    df = pd.read_csv(BytesIO(response['Body'].read()))

    logger.info(f"入力データの形状: {df.shape}")

    logger.info("データを訓練セットと検証セットに分割します。")
    train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)

    os.makedirs(args.output_train, exist_ok=True)
    os.makedirs(args.output_validation, exist_ok=True)

    logger.info("訓練データを保存します。")
    train_df.to_csv(os.path.join(args.output_train, "train.csv"), index=False)
    logger.info("検証データを保存します。")
    val_df.to_csv(os.path.join(args.output_validation, "validation.csv"), index=False)
    logger.info("前処理が完了しました。")

"""

with open('preprocessing.py', 'w') as f:
    f.write(processing_script)





# 前処理ステップの定義
script_processor = ScriptProcessor(
    image_uri=sagemaker.image_uris.retrieve(framework='sklearn', 
                                            region=region,
                                            version='0.23-1'),
    command=['python3'],
    instance_type=processing_instance_type,
    instance_count=1,
    base_job_name='lightgbm-preprocessing',
    role=role,
    sagemaker_session=sagemaker_session
)

processing_step = ProcessingStep(
    name='Preprocessing',
    processor=script_processor,
    inputs=[],  # ProcessingInputは不要
    outputs=[
        ProcessingOutput(output_name='train_data', source='/opt/ml/processing/train'),
        ProcessingOutput(output_name='validation_data', source='/opt/ml/processing/validation'),
    ],
    code='preprocessing.py',
    job_arguments=[
        "--s3-bucket", default_bucket,
        "--s3-key", "lightgbm-pipeline/input/data.csv"
    ]
)














pipeline = Pipeline(
    name='LightGBM-Pipeline-Optuna',
    parameters=[
        processing_instance_type,
        training_instance_type,
        model_approval_status,
    ],
    steps=[processing_step,],
    sagemaker_session=sagemaker_session,
)





# パイプラインの作成と実行
pipeline.upsert(role_arn=role)
execution = pipeline.start()


execution.list_steps()



